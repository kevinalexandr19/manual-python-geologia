{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba89c2d-e4d1-4f20-988f-398453628da1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align=\"center\">\n",
    "    <span style=\"font-size:30px\">\n",
    "        <strong>\n",
    "            <!-- Símbolo de Python -->\n",
    "            <img\n",
    "                src=\"https://cdn3.emoji.gg/emojis/1887_python.png\"\n",
    "                style=\"margin-bottom:-5px\"\n",
    "                width=\"30px\" \n",
    "                height=\"30px\"\n",
    "            >\n",
    "            <!-- Título -->\n",
    "            Python para Geólogos\n",
    "            <!-- Versión -->\n",
    "            <img \n",
    "                src=\"https://img.shields.io/github/release/kevinalexandr19/manual-python-geologia.svg?style=flat&label=&color=blue\"\n",
    "                style=\"margin-bottom:-2px\" \n",
    "                width=\"40px\"\n",
    "            >\n",
    "        </strong>\n",
    "    </span>\n",
    "    <br>\n",
    "    <span>\n",
    "        <!-- Github del proyecto -->\n",
    "        <a href=\"https://github.com/kevinalexandr19/manual-python-geologia\" target=\"_blank\">\n",
    "            <img src=\"https://img.shields.io/github/stars/kevinalexandr19/manual-python-geologia.svg?style=social&label=Github Repo\">\n",
    "        </a>\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Licencia -->\n",
    "        <img src=\"https://img.shields.io/github/license/kevinalexandr19/manual-python-geologia.svg?color=forestgreen\">\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Release date -->\n",
    "        <img src=\"https://img.shields.io/github/release-date/kevinalexandr19/manual-python-geologia?color=gold\">\n",
    "    </span>\n",
    "    <br>\n",
    "    <span>\n",
    "        <!-- Perfil de LinkedIn -->\n",
    "        <a target=\"_blank\" href=\"https://www.linkedin.com/in/kevin-alexander-gomez/\">\n",
    "            <img src=\"https://img.shields.io/badge/-Kevin Alexander Gomez-5eba00?style=social&logo=linkedin\">\n",
    "        </a>\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Perfil de Github -->\n",
    "        <a target=\"_blank\" href=\"https://github.com/kevinalexandr19\">\n",
    "            <img src=\"https://img.shields.io/github/followers/kevinalexandr19.svg?style=social&label=kevinalexandr19&maxAge=2592000\">\n",
    "        </a>\n",
    "    </span>\n",
    "    <br>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eda6ab-3fa9-4652-9e78-2023fc5a1e42",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:lightgreen; font-size:25px\">**PG201 - Aprendizaje supervisado**</span>\n",
    "\n",
    "Bienvenido al curso!!!\n",
    "\n",
    "Vamos a revisar diferentes algoritmos de <span style=\"color:gold\">aprendizaje supervisado</span> y su aplicación en Geología. <br>\n",
    "Es necesario que tengas un conocimiento previo en programación con Python, álgebra lineal, estadística y geología.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f752c8e-8060-4093-9ba9-0f2619d396d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:gold; font-size:20px\">**Árboles de decisión (DT)**</span>\n",
    "\n",
    "***\n",
    "- [¿Qué es un árbol de decisión?](#parte-1)\n",
    "- [Árboles de decisión en Python](#parte-2)\n",
    "- [¿Podemos visualizar un árbol de decisión?](#parte-3)\n",
    "- [Evaluación del modelo](#parte-4)\n",
    "- [Entropía y ganancia de la información](#parte-5)\n",
    "- [En conclusión...](#parte-6)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef8a2e-3f26-411f-b624-5b7fc1c52ff2",
   "metadata": {},
   "source": [
    "<a id=\"parte-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14023483-8dfb-4dee-ba17-8c3d6eaa8d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**¿Qué es un árbol de decisión?**</span>\n",
    "***\n",
    "\n",
    "Un árbol de decisión es un modelo predictivo utilizado tanto para clasificación como para regresión. Se basa en dividir el espacio de características en subconjuntos homogéneos mediante la aplicación de reglas de decisión, lo que resulta en una estructura de árbol. Cada nodo interno del árbol representa una prueba en una característica, cada rama representa el resultado de la prueba, y cada hoja representa una decisión final o una predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e4759-5212-41f0-ae2f-3ae5b0e7eda9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"resources/decision_tree.png\" alt=\"Árbol de decisión\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec7f51-1d4d-490c-9749-b72f47277cab",
   "metadata": {},
   "source": [
    "El árbol de decisión se construye de manera recursiva, dividiendo el conjunto de datos en función de una característica y un umbral que maximicen la \"pureza\" de los subconjuntos resultantes. La métrica de pureza más común es la impureza de Gini o la entropía para problemas de clasificación, y el error cuadrático medio para problemas de regresión.\n",
    "\n",
    "En cada nodo, se evalúan todas las características y se selecciona la división (característica y umbral) que minimice la impureza en los subconjuntos hijos. Esto se hace utilizando una métrica como la ganancia de información o la reducción de impureza de Gini.\n",
    "\n",
    "> <span style=\"color:gold\">**¿Qué es la impureza de Gini?**</span>\n",
    ">\n",
    "> La impureza de Gini es un criterio utilizado para evaluar la calidad de una división en los nodos de un árbol de decisión dentro del contexto de los **árboles de clasificación y regresión (CART)**, un modelo introducido por Leo Breiman.\n",
    "> \n",
    "> Este índice mide qué tan a menudo un elemento seleccionado al azar sería identificado incorrectamente si se le etiquetase de acuerdo con la distribución de etiquetas en el conjunto. En otras palabras, evalúa la probabilidad de que un atributo sea clasificado erróneamente si se escoge al azar según la distribución observada en el subconjunto.\n",
    "> \n",
    "> Un valor de impureza de Gini de 0 indica la pureza perfecta, es decir, todos los casos en el nodo pertenecen a una sola clase, mientras que valores más altos indican mayor mezcla de clases dentro del nodo. En la práctica, al construir un árbol de decisión CART, el objetivo es minimizar la impureza de Gini al elegir el mejor atributo para dividir los datos en cada paso, buscando aquellos puntos de división que resulten en subconjuntos lo más homogéneos posible respecto a la variable objetivo.\n",
    "> \n",
    "> La fórmula de la impureza de Gini para un nodo es:\n",
    ">\n",
    "> <center>\n",
    "    $$\n",
    "    \\Large Gini = 1 - \\sum_{i=1}^{c} p_i^2\n",
    "    $$\n",
    "> </center>\n",
    "> \n",
    "> Donde:\n",
    "> - $p_{i}$ es la proporción de elementos de clase $i$ en el nodo\n",
    "> - $c$ es el número de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5ca99-3071-4390-b8a2-eed7d70b3f5e",
   "metadata": {},
   "source": [
    "Este proceso de división continúa hasta que se cumpla una condición de parada, como alcanzar un número mínimo de muestras en un nodo, o que las divisiones ya no aporten mejora significativa en la pureza.\n",
    "\n",
    "Para realizar una predicción, se toma una nueva observación y se la pasa a través del árbol, comenzando desde la raíz y siguiendo las ramas según las reglas de decisión en cada nodo, hasta llegar a una hoja. El valor de la hoja es la predicción final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6203c4-45c6-411a-bcab-202e848d3743",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"resources/decision_tree_geology.png\" alt=\"Árbol de decisión geológico\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42daf84-8039-40d4-b4da-62b11229a2f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "Los árboles de decisión suelen preferir estructuras más simples, lo que está en línea con el principio de la Navaja de Occam. Este principio dice que \"la solución más simple suele ser la correcta\". En el caso de los árboles de decisión, esto implica que debemos evitar hacer el modelo más complejo de lo necesario, ya que las soluciones más sencillas suelen ser las más efectivas.\n",
    "\n",
    "Para limitar la complejidad y evitar el sobreajuste en los árboles de decisión, se utiliza una técnica llamada <span style=\"color:#43c6ac\">poda (pruning)</span>. Esta técnica elimina las ramas del árbol que dependen de atributos menos importantes. Después de podar el árbol, se puede evaluar la eficacia del modelo utilizando validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e045d-29c0-493d-acd7-9d8b73b25b41",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"color:gold\">**Ventajas y limitaciones del modelo Decision Tree** </span>\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Interpretabilidad:** </span> <br>\n",
    "Uno de los mayores beneficios de los árboles de decisión es que son fáciles de entender e interpretar. La estructura de árbol permite visualizar cómo se toman las decisiones, lo que los hace útiles en situaciones donde la explicabilidad es importante.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**No linealidad:** </span> <br>\n",
    "Los árboles de decisión no requieren que las relaciones entre características sean lineales, lo que los hace adecuados para modelar relaciones complejas en los datos.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Manejo de diferentes tipos de datos:** </span> <br>\n",
    "Pueden manejar tanto variables numéricas como categóricas sin la necesidad de transformar previamente los datos, a diferencia de otros algoritmos que solo operan sobre variables numéricas.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**No requiere escalamiento de características:** </span> <br>\n",
    "A diferencia de modelos como las redes neuronales y los modelos de soporte vectorial, los árboles de decisión no requieren que las características sean escaladas o normalizadas.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Eficiente con grandes datasets:** </span> <br>\n",
    "Los árboles pueden manejar grandes volúmenes de datos con una configuración computacional razonablemente buena.\n",
    "\n",
    "- <span style=\"color:orange\">**Sobreajuste:** </span> <br>\n",
    "Los árboles de decisión son propensos al sobreajuste, especialmente si el árbol es muy profundo. Esto ocurre porque el modelo puede terminar aprendiendo demasiado específicamente los detalles y el ruido del conjunto de datos de entrenamiento.\n",
    "\n",
    "- <span style=\"color:orange\">**Estabilidad:** </span> <br>\n",
    "Pequeñas variaciones en los datos pueden resultar en un árbol de decisión completamente diferente. Esto es debido a la naturaleza jerárquica del aprendizaje en los árboles de decisión, donde cada decisión tomada al principio afecta a los resultados subsecuentes de manera significativa.\n",
    "\n",
    "- <span style=\"color:orange\">**Problemas con datos desbalanceados:** </span> <br>\n",
    "Los árboles de decisión pueden crear árboles sesgados si algunas clases dominan. Esto es especialmente cierto sin técnicas adecuadas de preprocesamiento o ajuste de parámetros para manejar el desbalance.\n",
    "\n",
    "- <span style=\"color:orange\">**Dificultades con tareas de regresión que requieren extrapolación:** </span> <br>\n",
    "Los árboles de decisión no son efectivos en predecir resultados fuera del rango de los datos de entrenamiento, lo que limita su utilidad en algunos tipos de tareas de regresión.\n",
    "\n",
    "- <span style=\"color:orange\">**Heurísticas para divisiones pueden ser ineficientes para algunas tareas:** </span> <br>\n",
    "Las reglas de división están basadas en heurísticas como la maximización de la ganancia de información o la reducción de la impureza y no siempre resultan en la división óptima, especialmente en espacios de características complejas o correlacionadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7c5b5-2a99-497a-904f-53e9d40ded11",
   "metadata": {},
   "source": [
    "> <span style=\"color:gold\">**¿Es posible emplear múltiples árboles de decisión para realizar una predicción?** </span> <br>\n",
    "> Una manera de mejorar la exactitud de los árboles de decisión es mediante la creación de un conjunto de estos a través del algoritmo de [Random Forest](pg201c_randomforest.ipynb). Este enfoque permite obtener predicciones más exactas, especialmente cuando los árboles que componen el bosque no están correlacionados entre sí.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e42f48-0201-43e0-b5e9-65b014e903d2",
   "metadata": {},
   "source": [
    "<a id=\"parte-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0ac0e-c888-4b50-8e3d-299d43c5c9a5",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**Árboles de decisión en Python**</span>\n",
    "***\n",
    "\n",
    "Empezaremos importando `pandas` para cargar el archivo `rocas.csv`. También importaremos algunas funciones de Sci-Kit Learn:\n",
    "> **Sci-Kit Learn** es una librería utilizada para construir algoritmos de machine learning, la referenciamos dentro de Python como `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce8d91-7aab-459a-91f8-f3c7adc503dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier      # Modelo de Árbol de Decisión para clasificación\n",
    "from sklearn.model_selection import train_test_split # Función para dividir los datos de entrenamiento y prueba\n",
    "from sklearn.metrics import accuracy_score           # Función para medir la exactitud del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f98fa-d079-4510-b1b1-dce369db0d4f",
   "metadata": {},
   "source": [
    "Cargamos el archivo `rocas.csv` ubicado en la carpeta `files`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bf9c9-6060-43d1-afbb-0ce400a3349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"files/rocas.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96799df-2f01-439f-9637-426023dbe54b",
   "metadata": {},
   "source": [
    "Tenemos 22437 muestras de rocas volcánicas, con data geoquímica para diferentes elementos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f33f92-27e3-46bc-bc28-dcdfb357ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70576e7e-0feb-44b1-8c34-79cc5d3b2b68",
   "metadata": {},
   "source": [
    "También tenemos una columna categórica con 4 clases de rocas volcánicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37845706-aa0e-47ec-afdb-8290853a6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Nombre\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf022f1-cab0-4f24-be2c-b4ca2ca0a85f",
   "metadata": {},
   "source": [
    "Una de las ventajas de usar el modelo Decision Tree, es que no tenemos que transformar los datos.\n",
    "\n",
    "Separaremos directamente las columnas de la siguiente forma:\n",
    "\n",
    "- `X (features)` : contiene la información numérica de concentraciones geoquímicas, usada para entrenar y probar el modelo.\n",
    "- `y (target)` : contiene la información de la columna objetivo, es decir, la variable a predecir.\n",
    "\n",
    "Usaremos el atributo `values` del DataFrame para convertir la información en arreglos de Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c3694-4815-4a04-b3f4-e0991f708129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"Nombre\", axis=1)   # Features\n",
    "y = data[\"Nombre\"]                # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa2665-f3e4-4da9-86b0-6beb25974119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las columnas de features\n",
    "print(\"Features:\")\n",
    "print(\"----------\")\n",
    "print(X.values)\n",
    "print(\"\")\n",
    "\n",
    "# Mostramos la columna objetivo\n",
    "print(\"Target:\")\n",
    "print(\"----------\")\n",
    "print(y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579e5db-5f2b-443c-b6cf-d09fcb3ff6dc",
   "metadata": {},
   "source": [
    "Una vez separado los datos, procedemos a separar la data de entrenamiento y de prueba usando la función `train_test_split`:\n",
    "> El parámetro `test_size=0.25` representa la fracción de la data que será asignada al conjunto de prueba. <br>\n",
    "> También asignaremos un valor a `random_state` para que el resultado sea reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aff931-d40a-4cd1-b5d7-c1f21f835a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Tamaño de X_train: {X_train.shape}\")\n",
    "print(f\"Tamaño de X_test: {X_test.shape}\")\n",
    "print(f\"Tamaño de y_train: {y_train.shape}\")\n",
    "print(f\"Tamaño de y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb241b-c865-4880-88d6-f735a7f2d381",
   "metadata": {},
   "source": [
    "Ahora, inicializaremos el modelo `DecisionTreeClassifier` y estableceremos los siguientes hiperparámetros:\n",
    "- `criterion`: hace referencia al criterio de división de los nodos, en este caso usaremos el criterio de impureza de Gini.\n",
    "- `max_depth`: establece la profundidad del árbol de decisión, en este caso será igual a 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2685c65-8a13-46c0-9d27-60862bbb4f95",
   "metadata": {},
   "source": [
    "Para explicar algunos conceptos importantes, empezaremos usando diferentes valores de `max_depth`, y evaluaremos en cada caso los resultados de exactitud.\n",
    "\n",
    "Primero, crearemos un modelo `low_model` con un `max_depth` de 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a1561-e5c4-495c-8c23-c3db6acd89fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de Random Forest\n",
    "low_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=1, random_state=2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "low_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a21d79-35bd-4078-bfb9-7847b15a9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del modelo\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=low_model.predict(X_train))\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=low_model.predict(X_test))\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Decision Tree - Low Model\")\n",
    "print(\"-------------------------\")\n",
    "print(f\"Accuracy Score - Entrenamiento: {acc_train:.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {acc_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a774c-5aa7-45b4-a5b9-7ad9d863c167",
   "metadata": {},
   "source": [
    "Notamos una exactitud de tan solo el 54% tanto para el entrenamiento como para la prueba: nos encontramos ante un caso de subajuste (Underfitting).\n",
    "\n",
    "> <span style=\"color:gold\">**¿Qué es el subajuste o Underfitting?**</span>\n",
    "> \n",
    "> El underfitting ocurre cuando un modelo es demasiado simple para aprender la estructura subyacente de los datos.\n",
    ">\n",
    "> Como resultado, el modelo puede no captar las tendencias adecuadas en los datos, lo que lleva a:\n",
    ">\n",
    "> - <span style=\"color:orange\">Pobre rendimiento tanto en el conjunto de entrenamiento como en el de prueba:</span> <br>\n",
    "> Esto indica que el modelo no ha aprendido suficientemente los datos y, por lo tanto, no puede realizar predicciones precisas ni siquiera en los datos sobre los que fue entrenado.\n",
    ">\n",
    "> - <span style=\"color:orange\">Falta de adaptación a los datos:</span> <br>\n",
    "> Generalmente, esto es resultado de un modelo demasiado simple con muy pocos parámetros o características consideradas, que no captura la complejidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e0348-a83d-4629-a3ea-ba9137be8a9b",
   "metadata": {},
   "source": [
    "Ahora, usaremos otro modelo `high_model` con `max_depth` de 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb1fdf-cec2-41f9-9f30-e9ffc45b781e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Crear el modelo de Random Forest\n",
    "high_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=12, random_state=2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "high_model.fit(X_train, y_train)\n",
    "\n",
    "# Resultados del modelo\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=high_model.predict(X_train))\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=high_model.predict(X_test))\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Decision Tree - Low Model\")\n",
    "print(\"-------------------------\")\n",
    "print(f\"Accuracy Score - Entrenamiento: {acc_train:.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {acc_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798d690-8708-47e5-ab45-67656d3cc058",
   "metadata": {},
   "source": [
    "En este caso, nos encontramos ante un sobreajuste (Overfitting), debido a que la exactitud en el entrenamiento (95%) es mayor comparado a la exactitud de la prueba (88%).\n",
    "\n",
    "> <span style=\"color:gold\">**¿Qué es el sobreajuste o Overfitting?**</span>\n",
    "> \n",
    "> El overfitting ocurre cuando un modelo de aprendizaje automático está demasiado ajustado a los datos de entrenamiento, es decir, cuando aprende los detalles y el ruido del conjunto de entrenamiento hasta tal punto que impacta negativamente su rendimiento en datos nuevos o no vistos.\n",
    ">\n",
    "> Un modelo sobreajustado tiene las siguientes características:\n",
    "> - <span style=\"color:orange\">Alta exactitud en el conjunto de entrenamiento:</span> <br>\n",
    "> El modelo funciona excepcionalmente bien en el conjunto de entrenamiento, pero...\n",
    ">\n",
    "> - <span style=\"color:orange\">Pobre generalización a nuevos datos:</span> <br>\n",
    "> Su rendimiento decae significativamente cuando se enfrenta a datos no vistos, lo cual es un indicativo de que ha aprendido a responder a las peculiaridades y al ruido de los datos de entrenamiento en lugar de captar tendencias generalizables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee7396-8de4-4ad9-9c3d-5df14ed23a03",
   "metadata": {},
   "source": [
    "Vamos a entrenar un modelo intermedio entre estos casos, uno que se ajuste al modelo de manera correcta:\n",
    "> Seleccionaremos `max_depth` igual a 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c350-4040-4545-b78a-533dcb69240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fa4e2-e6c0-4b70-afb1-b761ccfb7217",
   "metadata": {},
   "source": [
    "Procederemos ahora a entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85eab57-2a95-4455-8f82-38288eb47a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de árbol de decisión\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3925b-2abe-4f82-a41a-3532bfe9c83d",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, evaluaremos su exactitud usando la función `accuracy_score`:\n",
    "\n",
    "> La <span style=\"color:#43c6ac\">exactitud (accuracy)</span> representa el porcentaje de predicciones que fueron correctas. <br>\n",
    "> El parámetro `y_true` representa la data que se busca obtener y `y_pred` es la predicción realizada por el modelo. <br>\n",
    "> Para predecir valores con el modelo, tenemos que usar el método `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1158aa9-8132-444e-9deb-2fa02ec4dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train) # Predicción del modelo con X_train\n",
    "y_test_pred = model.predict(X_test)   # Predicción del modelo con X_test\n",
    "\n",
    "print(f\"Accuracy Score - Entrenamiento: {accuracy_score(y_true=y_train, y_pred=y_train_pred):.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {accuracy_score(y_true=y_test, y_pred=y_test_pred):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182c8f0-c00e-42b0-bb60-21d4e559b972",
   "metadata": {},
   "source": [
    "El modelo de arbol de decisión ha obtenido una alta exactitud (aprox. 87%) para discriminar diferentes clases de rocas volcánicas. <br>\n",
    "Al tener una exactitud alta en el entrenamiento y de manera similar para la prueba, podemos concluir que no existe sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39861c32-b4b7-4063-b1df-92e45875fd03",
   "metadata": {},
   "source": [
    "Ahora, observaremos la importancia de cada columna usando el atributo `feature_importances_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016613f-722c-4e7a-ad15-cf097e4cbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = X.columns # Nombres de cada feature\n",
    "\n",
    "print(\"Importancia de atributos\")\n",
    "for col, imp in zip(x_cols, model.feature_importances_):\n",
    "    print(f\"{col}: {imp:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664ebe3-d445-45ba-997d-057acc6d481d",
   "metadata": {},
   "source": [
    "La <span style=\"color:#43c6ac\">importancia de atributos</span> nos ayuda a determinar qué variables son las más importantes para entrenar el modelo:\n",
    "- Observamos que la columna `SiO2` tiene una importancia muy alta (95%) comparada con el resto.\n",
    "- Las demás columnas, a excepción de `TiO2`(4%) y `FeOT` (<1%), son irrelevantes para el entrenamiento del modelo.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dc448-0726-4c0b-a7fc-51f49a938e93",
   "metadata": {},
   "source": [
    "<a id=\"parte-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a054efa-9645-4586-baa6-2d3d4c2071af",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**¿Podemos visualizar un árbol de decisión?**</span>\n",
    "***\n",
    "\n",
    "La respuesta es sí, y para esto, utilizaremos las funciones `export_text` y `plot_tree` del módulo `sklearn.tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80136f70-0888-499e-9533-0ba0dd01226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_text, plot_tree   # Funciones para graficar el árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38a8e9-6f71-4d22-9db1-16abf3d283ac",
   "metadata": {},
   "source": [
    "Crearemos una variable llamada `x_cols` para almacenar los nombres de las columnas de X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcbbd7-4a9a-43ea-ad53-259d25339d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamos los nombres de las columnas de X\n",
    "x_cols = list(X.columns)\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca2a12-922f-486f-bbef-4797112adc11",
   "metadata": {},
   "source": [
    "Exportamos los parámetros del árbol de decisión usando la función `export_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1b855-198b-4ff5-8b32-b6250fc17c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = export_text(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19647c-6869-438c-8ae1-3a3ab1d4203c",
   "metadata": {},
   "source": [
    "Ahora, procedemos a graficar el árbol de decisión usando la función `plot_tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ee882-64b1-4d69-9056-43e90a87213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "plot_tree(model, feature_names=x_cols, class_names=[\"Basalto\", \"Andesita\", \"Dacita\", \"Riolita\"],\n",
    "          fontsize=7, filled=True, proportion=True, node_ids=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b4e87-b707-446f-88f5-cfc48e1ad0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos la figura\n",
    "fig.savefig(\"files/decision_tree.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98565188-6f12-4a1c-a722-e675e2ca7fa3",
   "metadata": {},
   "source": [
    "Observando la estructura del Árbol de Decisión, notamos lo siguiente:\n",
    "\n",
    "- La sílice (SiO2) domina como variable inicial en el nodo raíz con un umbral de `SiO2 <= 52.195`, subrayando su rol clave en la determinación del tipo de roca. Su recurrencia en nodos sucesivos refuerza su predominio. Mientras tanto, `TiO2` y `FeOT` emergen como criterios secundarios, señalando una influencia comparativamente menor.\n",
    "\n",
    "- El modelo exhibe nodos con una pureza Gini notable y una alta proporción de muestras asignadas:\n",
    "    - `Nodo #4`: pureza Gini de 0.047, clasifica el 25.1% de las muestras como Andesita.\n",
    "    - `Nodo #10`: pureza Gini de 0.271, clasifica el 28.2% de las muestras como Basalto.\n",
    "    - `Nodo #13`: pureza Gini de 0.306, clasifica el 16.5% de las muestras como Dacita.\n",
    "    - `Nodo #14`: pureza Gini de 0.192, clasifica el 22.1% de las muestras como Riolita.\n",
    "\n",
    "> Esto es un indicador de que el modelo realiza una clasificación confiable en estas categorías.\n",
    "\n",
    "- Se identifican nodos con una menor cantidad de muestras y una mayor impureza Gini:\n",
    "    - `Nodo #3`: pureza Gini de 0.359, clasifica el 2.0% de las muestras como Andesita.\n",
    "    - `Nodo #6`: pureza Gini de 0.481, clasifica el 0.5% de las muestras como Basalto.\n",
    "    - `Nodo #7`: pureza Gini de 0.352, clasifica el 1.2% de las muestras como Andesita.\n",
    "    - `Nodo #11`, pureza Gini de 0.466, clasifica el 4.2% de las muestras como Andesita.\n",
    "\n",
    "> Esto es un indicador de áreas donde el modelo podría mejorar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c80d0-2875-4ec6-8b4d-ce4486128253",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875d215-e2a4-4bc0-a808-e2fbc62de2bf",
   "metadata": {},
   "source": [
    "<a id=\"parte-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107a8bd-2752-448c-bb71-f6de173327f6",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**Evaluación del modelo**</span>\n",
    "***\n",
    "\n",
    "Ahora que hemos entrenado un modelo de Árbol de Decisión, usaremos un gráfico que nos permita evaluar la clasificación de rocas volcánicas de acuerdo a la predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f380c4-e1ec-4ea9-8457-1bf20d8c6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ff8e6-33f2-4f26-b3ab-9f695bf4853a",
   "metadata": {},
   "source": [
    "Para facilitar la interpretación del gráfico y reducir la carga visual, seleccionaremos un subconjunto de muestras de nuestro conjunto de datos original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19af32-2539-43e6-97bc-5229f99d9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subconjunto de muestras\n",
    "sample = data.sample(2000, random_state=42)\n",
    "\n",
    "X_sample = sample.drop(columns=[\"Nombre\"])  # Columnas de features\n",
    "y_sample = sample[\"Nombre\"]                 # Columna target\n",
    "pred_sample = model.predict(X_sample)       # Predicción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde9ef2-ba9a-4f16-81c0-d60137b85fa0",
   "metadata": {},
   "source": [
    "Utilizaremos un mapa de colores específico para los gráficos. Esto facilitará la visualización y diferenciación de las distintas clases de rocas.\n",
    "\n",
    "> El parámetro `palette` es usado en Seaborn para asignar un mapa de colores para columnas categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3300f9e-3bc0-4c94-b227-952c6db91497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa de colores\n",
    "palette = {\"basalt\": \"blue\",\n",
    "           \"andesite\": \"green\",\n",
    "           \"dacite\": \"orange\",\n",
    "           \"rhyolite\": \"red\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d362b0e-7da5-4f08-b9fd-5fa0f826674d",
   "metadata": {},
   "source": [
    "Ahora, visualizaremos el resultado usando las dos variables importantes del modelo, `SiO2` y `TiO2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e0fa8-b3ae-4536-9a49-fb461ee3a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura principal\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(9, 5), sharey=True)\n",
    "\n",
    "# Diagrama de dispersión\n",
    "sns.scatterplot(ax=axs[0], data=X_sample, x=\"SiO2\", y=\"TiO2\", hue=y_sample,\n",
    "                edgecolor=\"k\", s=10, palette=palette, alpha=0.6)\n",
    "sns.scatterplot(ax=axs[1], data=X_sample, x=\"SiO2\", y=\"TiO2\", hue=pred_sample,\n",
    "                edgecolor=\"k\", s=10, palette=palette, alpha=0.6)\n",
    "\n",
    "# Título\n",
    "axs[0].set_title(\"Datos originales\")\n",
    "axs[1].set_title(\"Predicción del modelo\")\n",
    "fig.suptitle(\"Decision Tree - Clasificación de Rocas Volcánicas\")\n",
    "\n",
    "# Grilla y leyenda\n",
    "for ax in axs:\n",
    "    ax.grid(lw=0.5, c=\"k\", alpha=0.5)\n",
    "    ax.legend(title=\"Clases\", markerscale=2, edgecolor=\"k\")\n",
    "    ax.set_ylim(0, None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f266ac7-2e9f-4a8f-bda0-dc7d72e170e2",
   "metadata": {},
   "source": [
    "En el gráfico podemos observar lo siguiente:\n",
    "\n",
    "- El modelo ha logrado una exactitud aproximada del 87%, lo cual indica que la mayoría de los puntos han sido clasificados correctamente.\n",
    "\n",
    "- La predicción del modelo ha demarcado claramente los límites entre las diferentes clases. Estos límites, definidos por los nodos del árbol de decisión, se manifiestan como líneas de separación en el gráfico.\n",
    "\n",
    "Para una comprensión más profunda de cómo el modelo estructura los datos, podemos examinar otro gráfico que detalle específicamente estas líneas de separación. Este análisis adicional nos ayudará a entender mejor la distribución espacial de las clases y la lógica de clasificación del modelo.\n",
    "\n",
    "Empezaremos entrenando un nuevo modelo de Árbol de Decisión usando las dos variables importantes definidas por el modelo anterior: `SiO2` y `TiO2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f56d67-efae-45fa-8389-9728f4c31d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos los dos features más importantes\n",
    "features = [\"SiO2\", \"TiO2\"]\n",
    "\n",
    "# Entrenamos un árbol de decisión usando ambos features\n",
    "new_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
    "new_model.fit(X[features].values, y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d2b21-c6af-4ae3-b650-882640cd34d1",
   "metadata": {},
   "source": [
    "Ahora, crearemos una grilla de puntos que servirá como input para el gráfico:\n",
    "\n",
    "> La variable `Z` contendrá las predicciones del modelo sobre toda el área del gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171b2e0-2ec6-4016-acff-7c6609d2b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearemos una grilla de puntos para cubrir el espacio de datos\n",
    "x_min, x_max = X.SiO2.min(), X.SiO2.max()\n",
    "y_min, y_max = X.TiO2.min(), X.TiO2.max()\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Predicción del modelo sobre cada punto de la grilla\n",
    "Z = new_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38220db5-bcc2-4895-a7a1-6e3431cec046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el resultado\n",
    "print(\"Grilla de puntos - Predicción del modelo:\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f677cf7-b4d3-48d7-ac10-489438459a1f",
   "metadata": {},
   "source": [
    "Para graficar esta grilla, necesitaremos un nuevo mapa de colores:\n",
    "\n",
    "> Asignaremos un valor numérico a cada clase, y luego usaremos este valor como input para el mapa de colores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08348dd-87ae-438f-996d-6554da1370e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Creamos el mapa de colores (ordenado para cada clase)\n",
    "colors = [\"blue\", \"green\", \"orange\", \"red\"]  # Colores asignados\n",
    "cmap = ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66092ff-a27e-4af4-a2c2-25db832c0726",
   "metadata": {},
   "source": [
    "> La función `vectorize` de Numpy permite que una función se pueda aplicar a todos los elementos en un arreglo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e3e2b-37c0-4b48-9843-3c3d2f5ef03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario que asigna a cada clase un número entero\n",
    "category_to_number = {\"basalt\": 0,\n",
    "                      \"andesite\": 1,\n",
    "                      \"dacite\": 2,\n",
    "                      \"rhyolite\": 3}\n",
    "\n",
    "# Vectorización del diccionario\n",
    "Z_numerical = np.vectorize(category_to_number.get)(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae3a2d-80e5-4da6-9a86-67752b30fe03",
   "metadata": {},
   "source": [
    "La variable `Z_numerical` ahora contiene las predicciones del modelo transformadas a valores numéricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936ae78-f17c-480c-a115-d2bc96f6f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el resultado\n",
    "print(\"Grilla de puntos - Transformado a valores numéricos:\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(Z_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6dc7d-cf5e-4896-8a60-644df7a7ab55",
   "metadata": {},
   "source": [
    "Por último, desarrollaremos el gráfico:\n",
    "\n",
    "> Mostraremos las áreas de decisión definidas por el modelo. <br>\n",
    "> Para esto usaremos la función `contourf` de Matplotlib que pinta áreas en un espacio de puntos. <br>\n",
    "> También usaremos el subconjunto `sample` creado anteriormente para mostrar un diagrama de dispersión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe39973-05f7-4087-91b7-9c52f32e18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Usar contourf para plotear las áreas de decisión\n",
    "ax.contourf(xx, yy, Z_numerical, cmap=cmap, alpha=0.4)\n",
    "\n",
    "# Diagrama de dispersión\n",
    "sns.scatterplot(ax=ax, data=sample, x=\"SiO2\", y=\"TiO2\", hue=y_sample,\n",
    "                palette=palette, alpha=0.7, s=6, edgecolor=\"k\")\n",
    "\n",
    "# Texto y leyenda\n",
    "ax.set_title(\"Decision Tree - Áreas de decisión\")\n",
    "ax.set_xlabel(\"SiO2\")\n",
    "ax.set_ylabel(\"TiO2\")\n",
    "ax.legend(edgecolor=\"k\", markerscale=3, title=\"Clases\")\n",
    "\n",
    "# Grilla y límites\n",
    "ax.grid(lw=0.5, alpha=0.5, c=\"k\", ls=\"--\")\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_ylim(0, 6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86edc9f3-5459-43c8-a76f-a274b62a4bce",
   "metadata": {},
   "source": [
    "Una vez más, observamos que la mayoría de las muestras han sido clasificadas correctamente. Sin embargo, ahora es más evidente que algunas muestras están ubicadas en zonas incorrectas, es decir, están mal clasificadas.\n",
    "\n",
    "Esto se puede atribuir a la naturaleza de los límites de decisión impuestos por el modelo de Árbol de Decisión, los cuales son típicamente rectos y a menudo demasiado simplificados para capturar la complejidad real de los datos.\n",
    "\n",
    "> Para abordar este problema, se podrían explorar modelos más flexibles que permitan límites de decisión más irregulares y adaptativos, como las Máquinas de Soporte Vectorial (SVM) o los modelos basados en ensamblajes de árboles, como Random Forest o Gradient Boosting. <br>\n",
    ">\n",
    "> Estos modelos no solo ofrecen la posibilidad de ajustarse mejor a la distribución subyacente de los datos, sino que también pueden mejorar significativamente la exactitud de la clasificación en casos donde los patrones de los datos son más intrincados.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc7569-f76a-4dc3-9032-19ce54bb28a0",
   "metadata": {},
   "source": [
    "<a id=\"parte-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9c022-67af-4614-8454-748fa00e6b2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**Entropía y ganancia de información**</span>\n",
    "***\n",
    "\n",
    "Ahora que hemos explorado prácticamente cómo funcionan los árboles de decisión, es crucial profundizar en los conceptos fundamentales de entropía y ganancia de información. Estos principios son clave para comprender la mecánica subyacente de este modelo y cómo contribuyen a su eficacia:\n",
    "\n",
    "***\n",
    "<span style=\"color:gold\">**¿Qué es la entropía?**</span>\n",
    "\n",
    "La entropía se utiliza para cuantificar la impureza o incertidumbre presente en un conjunto de datos.\n",
    "\n",
    "La entropía alcanza su valor máximo cuando las clases en el conjunto de datos están perfectamente equilibradas (es decir, hay igual número de muestras de cada clase), lo que indica el mayor grado de incertidumbre o desorden. La entropía de un conjunto `𝑆` con `𝑘` clases diferentes se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{Entropía}(S) = -\\sum_{i=1}^k p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Donde `𝑝ᵢ` es la proporción de la clase `𝑖` en el conjunto `𝑆`.\n",
    "\n",
    "***\n",
    "<span style=\"color:gold\">**¿Qué es la ganancia de información?**</span>\n",
    "\n",
    "La ganancia de información es una medida que se deriva de la entropía y se utiliza para determinar cuál es el mejor atributo para dividir el conjunto de datos en cada paso de la construcción del árbol de decisión.\n",
    "\n",
    "Específicamente, <span style=\"color:#43c6ac\">indica cuánto \"mejora\" una división en términos de reducción de la impureza o incertidumbre.</span>\n",
    "\n",
    "La ganancia de información se calcula como la diferencia entre la entropía antes de la división y la suma ponderada de las entropías de cada subconjunto creado por la división, según el atributo seleccionado. Esto se puede expresar como:\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Información} = \\text{Entropía}(S) - \\sum_{v \\in \\text{Valores}} \\frac{|S_v|}{|S|} \\text{Entropía}(S_v)\n",
    "$$\n",
    "\n",
    "Donde `𝑆𝑣` es el subconjunto de `𝑆` para un valor específico `𝑣` del atributo, y `∣𝑆𝑣∣` y `∣𝑆∣` son las cantidades de muestras en los subconjuntos y en el conjunto original, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a3cef-6af8-4bf8-970b-d1a78574d0fb",
   "metadata": {},
   "source": [
    "En la construcción de un árbol de decisión, estos conceptos se aplican de la siguiente manera:\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Calculando la entropía del conjunto actual:**</span> <br>\n",
    "Se evalúa cuán desordenado o impuro es el conjunto de datos antes de cualquier división.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Evaluando ganancia de información para cada atributo:**</span> <br>\n",
    "Se calcula la ganancia de información para cada atributo disponible para determinar cuál de ellos proporciona la mayor reducción en la entropía, es decir, la mayor claridad en la clasificación después de la división.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Selección del mejor atributo:**</span> <br>\n",
    "El atributo que ofrece la máxima ganancia de información es seleccionado para realizar la división en ese nivel del árbol.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Repetición del proceso:**</span> <br>\n",
    "Este proceso se repite recursivamente para cada nuevo subconjunto hasta que los nodos contienen muestras suficientemente homogéneas o se alcanza una condición de parada predefinida.\n",
    "\n",
    "Estos principios garantizan que el árbol de decisión construido sea eficiente y efectivo, minimizando el número de divisiones necesarias para clasificar correctamente las muestras, lo que a su vez mejora la capacidad de generalización y eficiencia del modelo en tareas de predicción.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6dba6-a667-433c-b52e-88237e2257f4",
   "metadata": {},
   "source": [
    "<a id=\"parte-6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a14e0d-f144-49f8-bd95-7979897a5634",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**En conclusión...**</span>\n",
    "***\n",
    "\n",
    "- **Interpretación Basada en la Jerarquía:** <br>\n",
    "La estructura jerárquica de un árbol de decisión facilita la visualización de qué atributos son los más importantes, lo que los hace más fáciles de interpretar que otros modelos de aprendizaje automático. Gracias a la ganancia de información, podemos identificar de manera precisa y cuantitativa cuáles son los atributos más influyentes en la toma de decisiones.\n",
    "\n",
    "- **Flexibilidad en Aplicaciones:** <br>\n",
    "Los árboles de decisión no solo se limitan a la clasificación, sino que también se adaptan eficazmente a tareas de regresión. Esta versatilidad se debe en parte a cómo la entropía y la ganancia de información permiten manejar diferentes tipos de datos y estructuras de problema, haciendo que los árboles sean aplicables a una amplia gama de escenarios.\n",
    "\n",
    "- **Riesgo de Sobreajuste:** <br>\n",
    "Aunque los árboles de decisión pueden modelar complejidades detalladas en los datos, tienden a sobreajustarse, especialmente en árboles muy profundos. Este fenómeno ocurre cuando un árbol es demasiado específico a los datos de entrenamiento y no generaliza bien. La ganancia de información es crucial aquí; sin embargo, debe equilibrarse con técnicas como la poda para prevenir este sobreajuste.\n",
    "\n",
    "- **Sensibilidad a Variaciones en los Datos:** <br>\n",
    "Los árboles de decisión pueden ser altamente sensibles a pequeñas variaciones en los datos de entrenamiento. Cambios mínimos pueden resultar en árboles significativamente diferentes. Este es un reflejo de cómo las decisiones de división basadas en la entropía y la ganancia de información pueden alterarse con diferentes muestras de entrenamiento.\n",
    "\n",
    "- **Eficiencia en el Manejo de Datos:** <br>\n",
    "La capacidad de los árboles de decisión para manejar tanto atributos numéricos como categóricos directamente (sin la necesidad de transformaciones previas como el one-hot encoding) es una ventaja significativa. La entropía ayuda a manejar estas características al evaluar la impureza en las divisiones sin importar el tipo de dato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbccfa-e0aa-4cfa-ad65-83515bad4723",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
