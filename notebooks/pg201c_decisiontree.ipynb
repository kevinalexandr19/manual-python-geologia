{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba89c2d-e4d1-4f20-988f-398453628da1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align=\"center\">\n",
    "    <span style=\"font-size:30px\">\n",
    "        <strong>\n",
    "            <!-- S铆mbolo de Python -->\n",
    "            <img\n",
    "                src=\"https://cdn3.emoji.gg/emojis/1887_python.png\"\n",
    "                style=\"margin-bottom:-5px\"\n",
    "                width=\"30px\" \n",
    "                height=\"30px\"\n",
    "            >\n",
    "            <!-- T铆tulo -->\n",
    "            Python para Ge贸logos\n",
    "            <!-- Versi贸n -->\n",
    "            <img \n",
    "                src=\"https://img.shields.io/github/release/kevinalexandr19/manual-python-geologia.svg?style=flat&label=&color=blue\"\n",
    "                style=\"margin-bottom:-2px\" \n",
    "                width=\"40px\"\n",
    "            >\n",
    "        </strong>\n",
    "    </span>\n",
    "    <br>\n",
    "    <span>\n",
    "        <!-- Github del proyecto -->\n",
    "        <a href=\"https://github.com/kevinalexandr19/manual-python-geologia\" target=\"_blank\">\n",
    "            <img src=\"https://img.shields.io/github/stars/kevinalexandr19/manual-python-geologia.svg?style=social&label=Github Repo\">\n",
    "        </a>\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Licencia -->\n",
    "        <img src=\"https://img.shields.io/github/license/kevinalexandr19/manual-python-geologia.svg?color=forestgreen\">\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Release date -->\n",
    "        <img src=\"https://img.shields.io/github/release-date/kevinalexandr19/manual-python-geologia?color=gold\">\n",
    "    </span>\n",
    "    <br>\n",
    "    <span>\n",
    "        <!-- Perfil de LinkedIn -->\n",
    "        <a target=\"_blank\" href=\"https://www.linkedin.com/in/kevin-alexander-gomez/\">\n",
    "            <img src=\"https://img.shields.io/badge/-Kevin Alexander Gomez-5eba00?style=social&logo=linkedin\">\n",
    "        </a>\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Perfil de Github -->\n",
    "        <a target=\"_blank\" href=\"https://github.com/kevinalexandr19\">\n",
    "            <img src=\"https://img.shields.io/github/followers/kevinalexandr19.svg?style=social&label=kevinalexandr19&maxAge=2592000\">\n",
    "        </a>\n",
    "    </span>\n",
    "    <br>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eda6ab-3fa9-4652-9e78-2023fc5a1e42",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:lightgreen; font-size:25px\">**PG201 - Aprendizaje supervisado**</span>\n",
    "\n",
    "Bienvenido al curso!!!\n",
    "\n",
    "Vamos a revisar diferentes algoritmos de <span style=\"color:gold\">aprendizaje supervisado</span> y su aplicaci贸n en Geolog铆a. <br>\n",
    "Es necesario que tengas un conocimiento previo en programaci贸n con Python, 谩lgebra lineal, estad铆stica y geolog铆a.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f752c8e-8060-4093-9ba9-0f2619d396d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:gold; font-size:20px\">**rboles de decisi贸n (DT)**</span>\n",
    "\n",
    "***\n",
    "- [驴Qu茅 es un 谩rbol de decisi贸n?](#parte-1)\n",
    "- [rboles de decisi贸n en Python](#parte-2)\n",
    "- [驴Podemos visualizar un 谩rbol de decisi贸n?](#parte-3)\n",
    "- [Evaluaci贸n del modelo](#parte-4)\n",
    "- [Entrop铆a y ganancia de la informaci贸n](#parte-5)\n",
    "- [En conclusi贸n...](#parte-6)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef8a2e-3f26-411f-b624-5b7fc1c52ff2",
   "metadata": {},
   "source": [
    "<a id=\"parte-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14023483-8dfb-4dee-ba17-8c3d6eaa8d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**驴Qu茅 es un 谩rbol de decisi贸n?**</span>\n",
    "***\n",
    "\n",
    "Un 谩rbol de decisi贸n es un modelo predictivo utilizado tanto para clasificaci贸n como para regresi贸n. Se basa en dividir el espacio de caracter铆sticas en subconjuntos homog茅neos mediante la aplicaci贸n de reglas de decisi贸n, lo que resulta en una estructura de 谩rbol. Cada nodo interno del 谩rbol representa una prueba en una caracter铆stica, cada rama representa el resultado de la prueba, y cada hoja representa una decisi贸n final o una predicci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e4759-5212-41f0-ae2f-3ae5b0e7eda9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"resources/decision_tree.png\" alt=\"rbol de decisi贸n\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec7f51-1d4d-490c-9749-b72f47277cab",
   "metadata": {},
   "source": [
    "El 谩rbol de decisi贸n se construye de manera recursiva, dividiendo el conjunto de datos en funci贸n de una caracter铆stica y un umbral que maximicen la \"pureza\" de los subconjuntos resultantes. La m茅trica de pureza m谩s com煤n es la impureza de Gini o la entrop铆a para problemas de clasificaci贸n, y el error cuadr谩tico medio para problemas de regresi贸n.\n",
    "\n",
    "En cada nodo, se eval煤an todas las caracter铆sticas y se selecciona la divisi贸n (caracter铆stica y umbral) que minimice la impureza en los subconjuntos hijos. Esto se hace utilizando una m茅trica como la ganancia de informaci贸n o la reducci贸n de impureza de Gini.\n",
    "\n",
    "> <span style=\"color:gold\">**驴Qu茅 es la impureza de Gini?**</span>\n",
    ">\n",
    "> La impureza de Gini es un criterio utilizado para evaluar la calidad de una divisi贸n en los nodos de un 谩rbol de decisi贸n dentro del contexto de los **谩rboles de clasificaci贸n y regresi贸n (CART)**, un modelo introducido por Leo Breiman.\n",
    "> \n",
    "> Este 铆ndice mide qu茅 tan a menudo un elemento seleccionado al azar ser铆a identificado incorrectamente si se le etiquetase de acuerdo con la distribuci贸n de etiquetas en el conjunto. En otras palabras, eval煤a la probabilidad de que un atributo sea clasificado err贸neamente si se escoge al azar seg煤n la distribuci贸n observada en el subconjunto.\n",
    "> \n",
    "> Un valor de impureza de Gini de 0 indica la pureza perfecta, es decir, todos los casos en el nodo pertenecen a una sola clase, mientras que valores m谩s altos indican mayor mezcla de clases dentro del nodo. En la pr谩ctica, al construir un 谩rbol de decisi贸n CART, el objetivo es minimizar la impureza de Gini al elegir el mejor atributo para dividir los datos en cada paso, buscando aquellos puntos de divisi贸n que resulten en subconjuntos lo m谩s homog茅neos posible respecto a la variable objetivo.\n",
    "> \n",
    "> La f贸rmula de la impureza de Gini para un nodo es:\n",
    ">\n",
    "> <center>\n",
    "    $$\n",
    "    \\Large Gini = 1 - \\sum_{i=1}^{c} p_i^2\n",
    "    $$\n",
    "> </center>\n",
    "> \n",
    "> Donde:\n",
    "> - $p_{i}$ es la proporci贸n de elementos de clase $i$ en el nodo\n",
    "> - $c$ es el n煤mero de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5ca99-3071-4390-b8a2-eed7d70b3f5e",
   "metadata": {},
   "source": [
    "Este proceso de divisi贸n contin煤a hasta que se cumpla una condici贸n de parada, como alcanzar un n煤mero m铆nimo de muestras en un nodo, o que las divisiones ya no aporten mejora significativa en la pureza.\n",
    "\n",
    "Para realizar una predicci贸n, se toma una nueva observaci贸n y se la pasa a trav茅s del 谩rbol, comenzando desde la ra铆z y siguiendo las ramas seg煤n las reglas de decisi贸n en cada nodo, hasta llegar a una hoja. El valor de la hoja es la predicci贸n final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6203c4-45c6-411a-bcab-202e848d3743",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"resources/decision_tree_geology.png\" alt=\"rbol de decisi贸n geol贸gico\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42daf84-8039-40d4-b4da-62b11229a2f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "Los 谩rboles de decisi贸n suelen preferir estructuras m谩s simples, lo que est谩 en l铆nea con el principio de la Navaja de Occam. Este principio dice que \"la soluci贸n m谩s simple suele ser la correcta\". En el caso de los 谩rboles de decisi贸n, esto implica que debemos evitar hacer el modelo m谩s complejo de lo necesario, ya que las soluciones m谩s sencillas suelen ser las m谩s efectivas.\n",
    "\n",
    "Para limitar la complejidad y evitar el sobreajuste en los 谩rboles de decisi贸n, se utiliza una t茅cnica llamada <span style=\"color:#43c6ac\">poda (pruning)</span>. Esta t茅cnica elimina las ramas del 谩rbol que dependen de atributos menos importantes. Despu茅s de podar el 谩rbol, se puede evaluar la eficacia del modelo utilizando validaci贸n cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e045d-29c0-493d-acd7-9d8b73b25b41",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"color:gold\">**Ventajas y limitaciones del modelo Decision Tree** </span>\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Interpretabilidad:** </span> <br>\n",
    "Uno de los mayores beneficios de los 谩rboles de decisi贸n es que son f谩ciles de entender e interpretar. La estructura de 谩rbol permite visualizar c贸mo se toman las decisiones, lo que los hace 煤tiles en situaciones donde la explicabilidad es importante.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**No linealidad:** </span> <br>\n",
    "Los 谩rboles de decisi贸n no requieren que las relaciones entre caracter铆sticas sean lineales, lo que los hace adecuados para modelar relaciones complejas en los datos.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Manejo de diferentes tipos de datos:** </span> <br>\n",
    "Pueden manejar tanto variables num茅ricas como categ贸ricas sin la necesidad de transformar previamente los datos, a diferencia de otros algoritmos que solo operan sobre variables num茅ricas.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**No requiere escalamiento de caracter铆sticas:** </span> <br>\n",
    "A diferencia de modelos como las redes neuronales y los modelos de soporte vectorial, los 谩rboles de decisi贸n no requieren que las caracter铆sticas sean escaladas o normalizadas.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Eficiente con grandes datasets:** </span> <br>\n",
    "Los 谩rboles pueden manejar grandes vol煤menes de datos con una configuraci贸n computacional razonablemente buena.\n",
    "\n",
    "- <span style=\"color:orange\">**Sobreajuste:** </span> <br>\n",
    "Los 谩rboles de decisi贸n son propensos al sobreajuste, especialmente si el 谩rbol es muy profundo. Esto ocurre porque el modelo puede terminar aprendiendo demasiado espec铆ficamente los detalles y el ruido del conjunto de datos de entrenamiento.\n",
    "\n",
    "- <span style=\"color:orange\">**Estabilidad:** </span> <br>\n",
    "Peque帽as variaciones en los datos pueden resultar en un 谩rbol de decisi贸n completamente diferente. Esto es debido a la naturaleza jer谩rquica del aprendizaje en los 谩rboles de decisi贸n, donde cada decisi贸n tomada al principio afecta a los resultados subsecuentes de manera significativa.\n",
    "\n",
    "- <span style=\"color:orange\">**Problemas con datos desbalanceados:** </span> <br>\n",
    "Los 谩rboles de decisi贸n pueden crear 谩rboles sesgados si algunas clases dominan. Esto es especialmente cierto sin t茅cnicas adecuadas de preprocesamiento o ajuste de par谩metros para manejar el desbalance.\n",
    "\n",
    "- <span style=\"color:orange\">**Dificultades con tareas de regresi贸n que requieren extrapolaci贸n:** </span> <br>\n",
    "Los 谩rboles de decisi贸n no son efectivos en predecir resultados fuera del rango de los datos de entrenamiento, lo que limita su utilidad en algunos tipos de tareas de regresi贸n.\n",
    "\n",
    "- <span style=\"color:orange\">**Heur铆sticas para divisiones pueden ser ineficientes para algunas tareas:** </span> <br>\n",
    "Las reglas de divisi贸n est谩n basadas en heur铆sticas como la maximizaci贸n de la ganancia de informaci贸n o la reducci贸n de la impureza y no siempre resultan en la divisi贸n 贸ptima, especialmente en espacios de caracter铆sticas complejas o correlacionadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7c5b5-2a99-497a-904f-53e9d40ded11",
   "metadata": {},
   "source": [
    "> <span style=\"color:gold\">**驴Es posible emplear m煤ltiples 谩rboles de decisi贸n para realizar una predicci贸n?** </span> <br>\n",
    "> Una manera de mejorar la exactitud de los 谩rboles de decisi贸n es mediante la creaci贸n de un conjunto de estos a trav茅s del algoritmo de [Random Forest](pg201c_randomforest.ipynb). Este enfoque permite obtener predicciones m谩s exactas, especialmente cuando los 谩rboles que componen el bosque no est谩n correlacionados entre s铆.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e42f48-0201-43e0-b5e9-65b014e903d2",
   "metadata": {},
   "source": [
    "<a id=\"parte-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0ac0e-c888-4b50-8e3d-299d43c5c9a5",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**rboles de decisi贸n en Python**</span>\n",
    "***\n",
    "\n",
    "Empezaremos importando `pandas` para cargar el archivo `rocas.csv`. Tambi茅n importaremos algunas funciones de Sci-Kit Learn:\n",
    "> **Sci-Kit Learn** es una librer铆a utilizada para construir algoritmos de machine learning, la referenciamos dentro de Python como `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce8d91-7aab-459a-91f8-f3c7adc503dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier      # Modelo de rbol de Decisi贸n para clasificaci贸n\n",
    "from sklearn.model_selection import train_test_split # Funci贸n para dividir los datos de entrenamiento y prueba\n",
    "from sklearn.metrics import accuracy_score           # Funci贸n para medir la exactitud del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f98fa-d079-4510-b1b1-dce369db0d4f",
   "metadata": {},
   "source": [
    "Cargamos el archivo `rocas.csv` ubicado en la carpeta `files`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bf9c9-6060-43d1-afbb-0ce400a3349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"files/rocas.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96799df-2f01-439f-9637-426023dbe54b",
   "metadata": {},
   "source": [
    "Tenemos 22437 muestras de rocas volc谩nicas, con data geoqu铆mica para diferentes elementos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f33f92-27e3-46bc-bc28-dcdfb357ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70576e7e-0feb-44b1-8c34-79cc5d3b2b68",
   "metadata": {},
   "source": [
    "Tambi茅n tenemos una columna categ贸rica con 4 clases de rocas volc谩nicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37845706-aa0e-47ec-afdb-8290853a6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Nombre\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf022f1-cab0-4f24-be2c-b4ca2ca0a85f",
   "metadata": {},
   "source": [
    "Una de las ventajas de usar el modelo Decision Tree, es que no tenemos que transformar los datos.\n",
    "\n",
    "Separaremos directamente las columnas de la siguiente forma:\n",
    "\n",
    "- `X (features)` : contiene la informaci贸n num茅rica de concentraciones geoqu铆micas, usada para entrenar y probar el modelo.\n",
    "- `y (target)` : contiene la informaci贸n de la columna objetivo, es decir, la variable a predecir.\n",
    "\n",
    "Usaremos el atributo `values` del DataFrame para convertir la informaci贸n en arreglos de Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c3694-4815-4a04-b3f4-e0991f708129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"Nombre\", axis=1)   # Features\n",
    "y = data[\"Nombre\"]                # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa2665-f3e4-4da9-86b0-6beb25974119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las columnas de features\n",
    "print(\"Features:\")\n",
    "print(\"----------\")\n",
    "print(X.values)\n",
    "print(\"\")\n",
    "\n",
    "# Mostramos la columna objetivo\n",
    "print(\"Target:\")\n",
    "print(\"----------\")\n",
    "print(y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579e5db-5f2b-443c-b6cf-d09fcb3ff6dc",
   "metadata": {},
   "source": [
    "Una vez separado los datos, procedemos a separar la data de entrenamiento y de prueba usando la funci贸n `train_test_split`:\n",
    "> El par谩metro `test_size=0.25` representa la fracci贸n de la data que ser谩 asignada al conjunto de prueba. <br>\n",
    "> Tambi茅n asignaremos un valor a `random_state` para que el resultado sea reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aff931-d40a-4cd1-b5d7-c1f21f835a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Tama帽o de X_train: {X_train.shape}\")\n",
    "print(f\"Tama帽o de X_test: {X_test.shape}\")\n",
    "print(f\"Tama帽o de y_train: {y_train.shape}\")\n",
    "print(f\"Tama帽o de y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb241b-c865-4880-88d6-f735a7f2d381",
   "metadata": {},
   "source": [
    "Ahora, inicializaremos el modelo `DecisionTreeClassifier` y estableceremos los siguientes hiperpar谩metros:\n",
    "- `criterion`: hace referencia al criterio de divisi贸n de los nodos, en este caso usaremos el criterio de impureza de Gini.\n",
    "- `max_depth`: establece la profundidad del 谩rbol de decisi贸n, en este caso ser谩 igual a 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2685c65-8a13-46c0-9d27-60862bbb4f95",
   "metadata": {},
   "source": [
    "Para explicar algunos conceptos importantes, empezaremos usando diferentes valores de `max_depth`, y evaluaremos en cada caso los resultados de exactitud.\n",
    "\n",
    "Primero, crearemos un modelo `low_model` con un `max_depth` de 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a1561-e5c4-495c-8c23-c3db6acd89fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de Random Forest\n",
    "low_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=1, random_state=2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "low_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a21d79-35bd-4078-bfb9-7847b15a9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del modelo\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=low_model.predict(X_train))\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=low_model.predict(X_test))\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Decision Tree - Low Model\")\n",
    "print(\"-------------------------\")\n",
    "print(f\"Accuracy Score - Entrenamiento: {acc_train:.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {acc_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a774c-5aa7-45b4-a5b9-7ad9d863c167",
   "metadata": {},
   "source": [
    "Notamos una exactitud de tan solo el 54% tanto para el entrenamiento como para la prueba: nos encontramos ante un caso de subajuste (Underfitting).\n",
    "\n",
    "> <span style=\"color:gold\">**驴Qu茅 es el subajuste o Underfitting?**</span>\n",
    "> \n",
    "> El underfitting ocurre cuando un modelo es demasiado simple para aprender la estructura subyacente de los datos.\n",
    ">\n",
    "> Como resultado, el modelo puede no captar las tendencias adecuadas en los datos, lo que lleva a:\n",
    ">\n",
    "> - <span style=\"color:orange\">Pobre rendimiento tanto en el conjunto de entrenamiento como en el de prueba:</span> <br>\n",
    "> Esto indica que el modelo no ha aprendido suficientemente los datos y, por lo tanto, no puede realizar predicciones precisas ni siquiera en los datos sobre los que fue entrenado.\n",
    ">\n",
    "> - <span style=\"color:orange\">Falta de adaptaci贸n a los datos:</span> <br>\n",
    "> Generalmente, esto es resultado de un modelo demasiado simple con muy pocos par谩metros o caracter铆sticas consideradas, que no captura la complejidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e0348-a83d-4629-a3ea-ba9137be8a9b",
   "metadata": {},
   "source": [
    "Ahora, usaremos otro modelo `high_model` con `max_depth` de 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb1fdf-cec2-41f9-9f30-e9ffc45b781e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Crear el modelo de Random Forest\n",
    "high_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=12, random_state=2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "high_model.fit(X_train, y_train)\n",
    "\n",
    "# Resultados del modelo\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=high_model.predict(X_train))\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=high_model.predict(X_test))\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Decision Tree - Low Model\")\n",
    "print(\"-------------------------\")\n",
    "print(f\"Accuracy Score - Entrenamiento: {acc_train:.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {acc_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798d690-8708-47e5-ab45-67656d3cc058",
   "metadata": {},
   "source": [
    "En este caso, nos encontramos ante un sobreajuste (Overfitting), debido a que la exactitud en el entrenamiento (95%) es mayor comparado a la exactitud de la prueba (88%).\n",
    "\n",
    "> <span style=\"color:gold\">**驴Qu茅 es el sobreajuste o Overfitting?**</span>\n",
    "> \n",
    "> El overfitting ocurre cuando un modelo de aprendizaje autom谩tico est谩 demasiado ajustado a los datos de entrenamiento, es decir, cuando aprende los detalles y el ruido del conjunto de entrenamiento hasta tal punto que impacta negativamente su rendimiento en datos nuevos o no vistos.\n",
    ">\n",
    "> Un modelo sobreajustado tiene las siguientes caracter铆sticas:\n",
    "> - <span style=\"color:orange\">Alta exactitud en el conjunto de entrenamiento:</span> <br>\n",
    "> El modelo funciona excepcionalmente bien en el conjunto de entrenamiento, pero...\n",
    ">\n",
    "> - <span style=\"color:orange\">Pobre generalizaci贸n a nuevos datos:</span> <br>\n",
    "> Su rendimiento decae significativamente cuando se enfrenta a datos no vistos, lo cual es un indicativo de que ha aprendido a responder a las peculiaridades y al ruido de los datos de entrenamiento en lugar de captar tendencias generalizables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee7396-8de4-4ad9-9c3d-5df14ed23a03",
   "metadata": {},
   "source": [
    "Vamos a entrenar un modelo intermedio entre estos casos, uno que se ajuste al modelo de manera correcta:\n",
    "> Seleccionaremos `max_depth` igual a 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c350-4040-4545-b78a-533dcb69240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fa4e2-e6c0-4b70-afb1-b761ccfb7217",
   "metadata": {},
   "source": [
    "Procederemos ahora a entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85eab57-2a95-4455-8f82-38288eb47a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de 谩rbol de decisi贸n\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3925b-2abe-4f82-a41a-3532bfe9c83d",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, evaluaremos su exactitud usando la funci贸n `accuracy_score`:\n",
    "\n",
    "> La <span style=\"color:#43c6ac\">exactitud (accuracy)</span> representa el porcentaje de predicciones que fueron correctas. <br>\n",
    "> El par谩metro `y_true` representa la data que se busca obtener y `y_pred` es la predicci贸n realizada por el modelo. <br>\n",
    "> Para predecir valores con el modelo, tenemos que usar el m茅todo `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1158aa9-8132-444e-9deb-2fa02ec4dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train) # Predicci贸n del modelo con X_train\n",
    "y_test_pred = model.predict(X_test)   # Predicci贸n del modelo con X_test\n",
    "\n",
    "print(f\"Accuracy Score - Entrenamiento: {accuracy_score(y_true=y_train, y_pred=y_train_pred):.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {accuracy_score(y_true=y_test, y_pred=y_test_pred):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182c8f0-c00e-42b0-bb60-21d4e559b972",
   "metadata": {},
   "source": [
    "El modelo de arbol de decisi贸n ha obtenido una alta exactitud (aprox. 87%) para discriminar diferentes clases de rocas volc谩nicas. <br>\n",
    "Al tener una exactitud alta en el entrenamiento y de manera similar para la prueba, podemos concluir que no existe sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39861c32-b4b7-4063-b1df-92e45875fd03",
   "metadata": {},
   "source": [
    "Ahora, observaremos la importancia de cada columna usando el atributo `feature_importances_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016613f-722c-4e7a-ad15-cf097e4cbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = X.columns # Nombres de cada feature\n",
    "\n",
    "print(\"Importancia de atributos\")\n",
    "for col, imp in zip(x_cols, model.feature_importances_):\n",
    "    print(f\"{col}: {imp:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664ebe3-d445-45ba-997d-057acc6d481d",
   "metadata": {},
   "source": [
    "La <span style=\"color:#43c6ac\">importancia de atributos</span> nos ayuda a determinar qu茅 variables son las m谩s importantes para entrenar el modelo:\n",
    "- Observamos que la columna `SiO2` tiene una importancia muy alta (95%) comparada con el resto.\n",
    "- Las dem谩s columnas, a excepci贸n de `TiO2`(4%) y `FeOT` (<1%), son irrelevantes para el entrenamiento del modelo.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dc448-0726-4c0b-a7fc-51f49a938e93",
   "metadata": {},
   "source": [
    "<a id=\"parte-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a054efa-9645-4586-baa6-2d3d4c2071af",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**驴Podemos visualizar un 谩rbol de decisi贸n?**</span>\n",
    "***\n",
    "\n",
    "La respuesta es s铆, y para esto, utilizaremos las funciones `export_text` y `plot_tree` del m贸dulo `sklearn.tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80136f70-0888-499e-9533-0ba0dd01226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_text, plot_tree   # Funciones para graficar el 谩rbol de decisi贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38a8e9-6f71-4d22-9db1-16abf3d283ac",
   "metadata": {},
   "source": [
    "Crearemos una variable llamada `x_cols` para almacenar los nombres de las columnas de X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcbbd7-4a9a-43ea-ad53-259d25339d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamos los nombres de las columnas de X\n",
    "x_cols = list(X.columns)\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca2a12-922f-486f-bbef-4797112adc11",
   "metadata": {},
   "source": [
    "Exportamos los par谩metros del 谩rbol de decisi贸n usando la funci贸n `export_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1b855-198b-4ff5-8b32-b6250fc17c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = export_text(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19647c-6869-438c-8ae1-3a3ab1d4203c",
   "metadata": {},
   "source": [
    "Ahora, procedemos a graficar el 谩rbol de decisi贸n usando la funci贸n `plot_tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ee882-64b1-4d69-9056-43e90a87213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "plot_tree(model, feature_names=x_cols, class_names=[\"Basalto\", \"Andesita\", \"Dacita\", \"Riolita\"],\n",
    "          fontsize=7, filled=True, proportion=True, node_ids=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b4e87-b707-446f-88f5-cfc48e1ad0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos la figura\n",
    "fig.savefig(\"files/decision_tree.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98565188-6f12-4a1c-a722-e675e2ca7fa3",
   "metadata": {},
   "source": [
    "Observando la estructura del rbol de Decisi贸n, notamos lo siguiente:\n",
    "\n",
    "- La s铆lice (SiO2) domina como variable inicial en el nodo ra铆z con un umbral de `SiO2 <= 52.195`, subrayando su rol clave en la determinaci贸n del tipo de roca. Su recurrencia en nodos sucesivos refuerza su predominio. Mientras tanto, `TiO2` y `FeOT` emergen como criterios secundarios, se帽alando una influencia comparativamente menor.\n",
    "\n",
    "- El modelo exhibe nodos con una pureza Gini notable y una alta proporci贸n de muestras asignadas:\n",
    "    - `Nodo #4`: pureza Gini de 0.047, clasifica el 25.1% de las muestras como Andesita.\n",
    "    - `Nodo #10`: pureza Gini de 0.271, clasifica el 28.2% de las muestras como Basalto.\n",
    "    - `Nodo #13`: pureza Gini de 0.306, clasifica el 16.5% de las muestras como Dacita.\n",
    "    - `Nodo #14`: pureza Gini de 0.192, clasifica el 22.1% de las muestras como Riolita.\n",
    "\n",
    "> Esto es un indicador de que el modelo realiza una clasificaci贸n confiable en estas categor铆as.\n",
    "\n",
    "- Se identifican nodos con una menor cantidad de muestras y una mayor impureza Gini:\n",
    "    - `Nodo #3`: pureza Gini de 0.359, clasifica el 2.0% de las muestras como Andesita.\n",
    "    - `Nodo #6`: pureza Gini de 0.481, clasifica el 0.5% de las muestras como Basalto.\n",
    "    - `Nodo #7`: pureza Gini de 0.352, clasifica el 1.2% de las muestras como Andesita.\n",
    "    - `Nodo #11`, pureza Gini de 0.466, clasifica el 4.2% de las muestras como Andesita.\n",
    "\n",
    "> Esto es un indicador de 谩reas donde el modelo podr铆a mejorar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c80d0-2875-4ec6-8b4d-ce4486128253",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875d215-e2a4-4bc0-a808-e2fbc62de2bf",
   "metadata": {},
   "source": [
    "<a id=\"parte-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107a8bd-2752-448c-bb71-f6de173327f6",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**Evaluaci贸n del modelo**</span>\n",
    "***\n",
    "\n",
    "Ahora que hemos entrenado un modelo de rbol de Decisi贸n, usaremos un gr谩fico que nos permita evaluar la clasificaci贸n de rocas volc谩nicas de acuerdo a la predicci贸n del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f380c4-e1ec-4ea9-8457-1bf20d8c6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ff8e6-33f2-4f26-b3ab-9f695bf4853a",
   "metadata": {},
   "source": [
    "Para facilitar la interpretaci贸n del gr谩fico y reducir la carga visual, seleccionaremos un subconjunto de muestras de nuestro conjunto de datos original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19af32-2539-43e6-97bc-5229f99d9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subconjunto de muestras\n",
    "sample = data.sample(2000, random_state=42)\n",
    "\n",
    "X_sample = sample.drop(columns=[\"Nombre\"])  # Columnas de features\n",
    "y_sample = sample[\"Nombre\"]                 # Columna target\n",
    "pred_sample = model.predict(X_sample)       # Predicci贸n del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde9ef2-ba9a-4f16-81c0-d60137b85fa0",
   "metadata": {},
   "source": [
    "Utilizaremos un mapa de colores espec铆fico para los gr谩ficos. Esto facilitar谩 la visualizaci贸n y diferenciaci贸n de las distintas clases de rocas.\n",
    "\n",
    "> El par谩metro `palette` es usado en Seaborn para asignar un mapa de colores para columnas categ贸ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3300f9e-3bc0-4c94-b227-952c6db91497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa de colores\n",
    "palette = {\"basalt\": \"blue\",\n",
    "           \"andesite\": \"green\",\n",
    "           \"dacite\": \"orange\",\n",
    "           \"rhyolite\": \"red\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d362b0e-7da5-4f08-b9fd-5fa0f826674d",
   "metadata": {},
   "source": [
    "Ahora, visualizaremos el resultado usando las dos variables importantes del modelo, `SiO2` y `TiO2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e0fa8-b3ae-4536-9a49-fb461ee3a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura principal\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(9, 5), sharey=True)\n",
    "\n",
    "# Diagrama de dispersi贸n\n",
    "sns.scatterplot(ax=axs[0], data=X_sample, x=\"SiO2\", y=\"TiO2\", hue=y_sample,\n",
    "                edgecolor=\"k\", s=10, palette=palette, alpha=0.6)\n",
    "sns.scatterplot(ax=axs[1], data=X_sample, x=\"SiO2\", y=\"TiO2\", hue=pred_sample,\n",
    "                edgecolor=\"k\", s=10, palette=palette, alpha=0.6)\n",
    "\n",
    "# T铆tulo\n",
    "axs[0].set_title(\"Datos originales\")\n",
    "axs[1].set_title(\"Predicci贸n del modelo\")\n",
    "fig.suptitle(\"Decision Tree - Clasificaci贸n de Rocas Volc谩nicas\")\n",
    "\n",
    "# Grilla y leyenda\n",
    "for ax in axs:\n",
    "    ax.grid(lw=0.5, c=\"k\", alpha=0.5)\n",
    "    ax.legend(title=\"Clases\", markerscale=2, edgecolor=\"k\")\n",
    "    ax.set_ylim(0, None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f266ac7-2e9f-4a8f-bda0-dc7d72e170e2",
   "metadata": {},
   "source": [
    "En el gr谩fico podemos observar lo siguiente:\n",
    "\n",
    "- El modelo ha logrado una exactitud aproximada del 87%, lo cual indica que la mayor铆a de los puntos han sido clasificados correctamente.\n",
    "\n",
    "- La predicci贸n del modelo ha demarcado claramente los l铆mites entre las diferentes clases. Estos l铆mites, definidos por los nodos del 谩rbol de decisi贸n, se manifiestan como l铆neas de separaci贸n en el gr谩fico.\n",
    "\n",
    "Para una comprensi贸n m谩s profunda de c贸mo el modelo estructura los datos, podemos examinar otro gr谩fico que detalle espec铆ficamente estas l铆neas de separaci贸n. Este an谩lisis adicional nos ayudar谩 a entender mejor la distribuci贸n espacial de las clases y la l贸gica de clasificaci贸n del modelo.\n",
    "\n",
    "Empezaremos entrenando un nuevo modelo de rbol de Decisi贸n usando las dos variables importantes definidas por el modelo anterior: `SiO2` y `TiO2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f56d67-efae-45fa-8389-9728f4c31d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos los dos features m谩s importantes\n",
    "features = [\"SiO2\", \"TiO2\"]\n",
    "\n",
    "# Entrenamos un 谩rbol de decisi贸n usando ambos features\n",
    "new_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
    "new_model.fit(X[features].values, y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d2b21-c6af-4ae3-b650-882640cd34d1",
   "metadata": {},
   "source": [
    "Ahora, crearemos una grilla de puntos que servir谩 como input para el gr谩fico:\n",
    "\n",
    "> La variable `Z` contendr谩 las predicciones del modelo sobre toda el 谩rea del gr谩fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171b2e0-2ec6-4016-acff-7c6609d2b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearemos una grilla de puntos para cubrir el espacio de datos\n",
    "x_min, x_max = X.SiO2.min(), X.SiO2.max()\n",
    "y_min, y_max = X.TiO2.min(), X.TiO2.max()\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Predicci贸n del modelo sobre cada punto de la grilla\n",
    "Z = new_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38220db5-bcc2-4895-a7a1-6e3431cec046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el resultado\n",
    "print(\"Grilla de puntos - Predicci贸n del modelo:\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f677cf7-b4d3-48d7-ac10-489438459a1f",
   "metadata": {},
   "source": [
    "Para graficar esta grilla, necesitaremos un nuevo mapa de colores:\n",
    "\n",
    "> Asignaremos un valor num茅rico a cada clase, y luego usaremos este valor como input para el mapa de colores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08348dd-87ae-438f-996d-6554da1370e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Creamos el mapa de colores (ordenado para cada clase)\n",
    "colors = [\"blue\", \"green\", \"orange\", \"red\"]  # Colores asignados\n",
    "cmap = ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66092ff-a27e-4af4-a2c2-25db832c0726",
   "metadata": {},
   "source": [
    "> La funci贸n `vectorize` de Numpy permite que una funci贸n se pueda aplicar a todos los elementos en un arreglo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e3e2b-37c0-4b48-9843-3c3d2f5ef03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario que asigna a cada clase un n煤mero entero\n",
    "category_to_number = {\"basalt\": 0,\n",
    "                      \"andesite\": 1,\n",
    "                      \"dacite\": 2,\n",
    "                      \"rhyolite\": 3}\n",
    "\n",
    "# Vectorizaci贸n del diccionario\n",
    "Z_numerical = np.vectorize(category_to_number.get)(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae3a2d-80e5-4da6-9a86-67752b30fe03",
   "metadata": {},
   "source": [
    "La variable `Z_numerical` ahora contiene las predicciones del modelo transformadas a valores num茅ricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936ae78-f17c-480c-a115-d2bc96f6f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el resultado\n",
    "print(\"Grilla de puntos - Transformado a valores num茅ricos:\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(Z_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6dc7d-cf5e-4896-8a60-644df7a7ab55",
   "metadata": {},
   "source": [
    "Por 煤ltimo, desarrollaremos el gr谩fico:\n",
    "\n",
    "> Mostraremos las 谩reas de decisi贸n definidas por el modelo. <br>\n",
    "> Para esto usaremos la funci贸n `contourf` de Matplotlib que pinta 谩reas en un espacio de puntos. <br>\n",
    "> Tambi茅n usaremos el subconjunto `sample` creado anteriormente para mostrar un diagrama de dispersi贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe39973-05f7-4087-91b7-9c52f32e18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Usar contourf para plotear las 谩reas de decisi贸n\n",
    "ax.contourf(xx, yy, Z_numerical, cmap=cmap, alpha=0.4)\n",
    "\n",
    "# Diagrama de dispersi贸n\n",
    "sns.scatterplot(ax=ax, data=sample, x=\"SiO2\", y=\"TiO2\", hue=y_sample,\n",
    "                palette=palette, alpha=0.7, s=6, edgecolor=\"k\")\n",
    "\n",
    "# Texto y leyenda\n",
    "ax.set_title(\"Decision Tree - reas de decisi贸n\")\n",
    "ax.set_xlabel(\"SiO2\")\n",
    "ax.set_ylabel(\"TiO2\")\n",
    "ax.legend(edgecolor=\"k\", markerscale=3, title=\"Clases\")\n",
    "\n",
    "# Grilla y l铆mites\n",
    "ax.grid(lw=0.5, alpha=0.5, c=\"k\", ls=\"--\")\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_ylim(0, 6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86edc9f3-5459-43c8-a76f-a274b62a4bce",
   "metadata": {},
   "source": [
    "Una vez m谩s, observamos que la mayor铆a de las muestras han sido clasificadas correctamente. Sin embargo, ahora es m谩s evidente que algunas muestras est谩n ubicadas en zonas incorrectas, es decir, est谩n mal clasificadas.\n",
    "\n",
    "Esto se puede atribuir a la naturaleza de los l铆mites de decisi贸n impuestos por el modelo de rbol de Decisi贸n, los cuales son t铆picamente rectos y a menudo demasiado simplificados para capturar la complejidad real de los datos.\n",
    "\n",
    "> Para abordar este problema, se podr铆an explorar modelos m谩s flexibles que permitan l铆mites de decisi贸n m谩s irregulares y adaptativos, como las M谩quinas de Soporte Vectorial (SVM) o los modelos basados en ensamblajes de 谩rboles, como Random Forest o Gradient Boosting. <br>\n",
    ">\n",
    "> Estos modelos no solo ofrecen la posibilidad de ajustarse mejor a la distribuci贸n subyacente de los datos, sino que tambi茅n pueden mejorar significativamente la exactitud de la clasificaci贸n en casos donde los patrones de los datos son m谩s intrincados.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc7569-f76a-4dc3-9032-19ce54bb28a0",
   "metadata": {},
   "source": [
    "<a id=\"parte-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9c022-67af-4614-8454-748fa00e6b2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**Entrop铆a y ganancia de informaci贸n**</span>\n",
    "***\n",
    "\n",
    "Ahora que hemos explorado pr谩cticamente c贸mo funcionan los 谩rboles de decisi贸n, es crucial profundizar en los conceptos fundamentales de entrop铆a y ganancia de informaci贸n. Estos principios son clave para comprender la mec谩nica subyacente de este modelo y c贸mo contribuyen a su eficacia:\n",
    "\n",
    "***\n",
    "<span style=\"color:gold\">**驴Qu茅 es la entrop铆a?**</span>\n",
    "\n",
    "La entrop铆a se utiliza para cuantificar la impureza o incertidumbre presente en un conjunto de datos.\n",
    "\n",
    "La entrop铆a alcanza su valor m谩ximo cuando las clases en el conjunto de datos est谩n perfectamente equilibradas (es decir, hay igual n煤mero de muestras de cada clase), lo que indica el mayor grado de incertidumbre o desorden. La entrop铆a de un conjunto `` con `` clases diferentes se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{Entrop铆a}(S) = -\\sum_{i=1}^k p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Donde `岬` es la proporci贸n de la clase `` en el conjunto ``.\n",
    "\n",
    "***\n",
    "<span style=\"color:gold\">**驴Qu茅 es la ganancia de informaci贸n?**</span>\n",
    "\n",
    "La ganancia de informaci贸n es una medida que se deriva de la entrop铆a y se utiliza para determinar cu谩l es el mejor atributo para dividir el conjunto de datos en cada paso de la construcci贸n del 谩rbol de decisi贸n.\n",
    "\n",
    "Espec铆ficamente, <span style=\"color:#43c6ac\">indica cu谩nto \"mejora\" una divisi贸n en t茅rminos de reducci贸n de la impureza o incertidumbre.</span>\n",
    "\n",
    "La ganancia de informaci贸n se calcula como la diferencia entre la entrop铆a antes de la divisi贸n y la suma ponderada de las entrop铆as de cada subconjunto creado por la divisi贸n, seg煤n el atributo seleccionado. Esto se puede expresar como:\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Informaci贸n} = \\text{Entrop铆a}(S) - \\sum_{v \\in \\text{Valores}} \\frac{|S_v|}{|S|} \\text{Entrop铆a}(S_v)\n",
    "$$\n",
    "\n",
    "Donde `` es el subconjunto de `` para un valor espec铆fico `` del atributo, y `ｐｂ` y `ｐ` son las cantidades de muestras en los subconjuntos y en el conjunto original, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a3cef-6af8-4bf8-970b-d1a78574d0fb",
   "metadata": {},
   "source": [
    "En la construcci贸n de un 谩rbol de decisi贸n, estos conceptos se aplican de la siguiente manera:\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Calculando la entrop铆a del conjunto actual:**</span> <br>\n",
    "Se eval煤a cu谩n desordenado o impuro es el conjunto de datos antes de cualquier divisi贸n.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Evaluando ganancia de informaci贸n para cada atributo:**</span> <br>\n",
    "Se calcula la ganancia de informaci贸n para cada atributo disponible para determinar cu谩l de ellos proporciona la mayor reducci贸n en la entrop铆a, es decir, la mayor claridad en la clasificaci贸n despu茅s de la divisi贸n.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Selecci贸n del mejor atributo:**</span> <br>\n",
    "El atributo que ofrece la m谩xima ganancia de informaci贸n es seleccionado para realizar la divisi贸n en ese nivel del 谩rbol.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Repetici贸n del proceso:**</span> <br>\n",
    "Este proceso se repite recursivamente para cada nuevo subconjunto hasta que los nodos contienen muestras suficientemente homog茅neas o se alcanza una condici贸n de parada predefinida.\n",
    "\n",
    "Estos principios garantizan que el 谩rbol de decisi贸n construido sea eficiente y efectivo, minimizando el n煤mero de divisiones necesarias para clasificar correctamente las muestras, lo que a su vez mejora la capacidad de generalizaci贸n y eficiencia del modelo en tareas de predicci贸n.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6dba6-a667-433c-b52e-88237e2257f4",
   "metadata": {},
   "source": [
    "<a id=\"parte-6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a14e0d-f144-49f8-bd95-7979897a5634",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**En conclusi贸n...**</span>\n",
    "***\n",
    "\n",
    "- **Interpretaci贸n Basada en la Jerarqu铆a:** <br>\n",
    "La estructura jer谩rquica de un 谩rbol de decisi贸n facilita la visualizaci贸n de qu茅 atributos son los m谩s importantes, lo que los hace m谩s f谩ciles de interpretar que otros modelos de aprendizaje autom谩tico. Gracias a la ganancia de informaci贸n, podemos identificar de manera precisa y cuantitativa cu谩les son los atributos m谩s influyentes en la toma de decisiones.\n",
    "\n",
    "- **Flexibilidad en Aplicaciones:** <br>\n",
    "Los 谩rboles de decisi贸n no solo se limitan a la clasificaci贸n, sino que tambi茅n se adaptan eficazmente a tareas de regresi贸n. Esta versatilidad se debe en parte a c贸mo la entrop铆a y la ganancia de informaci贸n permiten manejar diferentes tipos de datos y estructuras de problema, haciendo que los 谩rboles sean aplicables a una amplia gama de escenarios.\n",
    "\n",
    "- **Riesgo de Sobreajuste:** <br>\n",
    "Aunque los 谩rboles de decisi贸n pueden modelar complejidades detalladas en los datos, tienden a sobreajustarse, especialmente en 谩rboles muy profundos. Este fen贸meno ocurre cuando un 谩rbol es demasiado espec铆fico a los datos de entrenamiento y no generaliza bien. La ganancia de informaci贸n es crucial aqu铆; sin embargo, debe equilibrarse con t茅cnicas como la poda para prevenir este sobreajuste.\n",
    "\n",
    "- **Sensibilidad a Variaciones en los Datos:** <br>\n",
    "Los 谩rboles de decisi贸n pueden ser altamente sensibles a peque帽as variaciones en los datos de entrenamiento. Cambios m铆nimos pueden resultar en 谩rboles significativamente diferentes. Este es un reflejo de c贸mo las decisiones de divisi贸n basadas en la entrop铆a y la ganancia de informaci贸n pueden alterarse con diferentes muestras de entrenamiento.\n",
    "\n",
    "- **Eficiencia en el Manejo de Datos:** <br>\n",
    "La capacidad de los 谩rboles de decisi贸n para manejar tanto atributos num茅ricos como categ贸ricos directamente (sin la necesidad de transformaciones previas como el one-hot encoding) es una ventaja significativa. La entrop铆a ayuda a manejar estas caracter铆sticas al evaluar la impureza en las divisiones sin importar el tipo de dato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbccfa-e0aa-4cfa-ad65-83515bad4723",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
