{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c46712-53ba-4bab-a787-ce9f5ac0c34f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align=\"center\">\n",
    "    <span style=\"font-size:30px\">\n",
    "        <strong>\n",
    "            <!-- Símbolo de Python -->\n",
    "            <img\n",
    "                src=\"https://cdn3.emoji.gg/emojis/1887_python.png\"\n",
    "                style=\"margin-bottom:-5px\"\n",
    "                width=\"30px\" \n",
    "                height=\"30px\"\n",
    "            >\n",
    "            <!-- Título -->\n",
    "            Python para Geólogos\n",
    "            <!-- Versión -->\n",
    "            <img \n",
    "                src=\"https://img.shields.io/github/release/kevinalexandr19/manual-python-geologia.svg?style=flat&label=&color=blue\"\n",
    "                style=\"margin-bottom:-2px\" \n",
    "                width=\"40px\"\n",
    "            >\n",
    "        </strong>\n",
    "    </span>\n",
    "    <br>\n",
    "    <span>\n",
    "        <!-- Github del proyecto -->\n",
    "        <a href=\"https://github.com/kevinalexandr19/manual-python-geologia\" target=\"_blank\">\n",
    "            <img src=\"https://img.shields.io/github/stars/kevinalexandr19/manual-python-geologia.svg?style=social&label=Github Repo\">\n",
    "        </a>\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Licencia -->\n",
    "        <img src=\"https://img.shields.io/github/license/kevinalexandr19/manual-python-geologia.svg?color=forestgreen\">\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Release date -->\n",
    "        <img src=\"https://img.shields.io/github/release-date/kevinalexandr19/manual-python-geologia?color=gold\">\n",
    "    </span>\n",
    "    <br>\n",
    "    <span>\n",
    "        <!-- Perfil de LinkedIn -->\n",
    "        <a target=\"_blank\" href=\"https://www.linkedin.com/in/kevin-alexander-gomez/\">\n",
    "            <img src=\"https://img.shields.io/badge/-Kevin Alexander Gomez-5eba00?style=social&logo=linkedin\">\n",
    "        </a>\n",
    "        &nbsp;&nbsp;\n",
    "        <!-- Perfil de Github -->\n",
    "        <a target=\"_blank\" href=\"https://github.com/kevinalexandr19\">\n",
    "            <img src=\"https://img.shields.io/github/followers/kevinalexandr19.svg?style=social&label=kevinalexandr19&maxAge=2592000\">\n",
    "        </a>\n",
    "    </span>\n",
    "    <br>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95676b8-7409-484b-806a-4efaf39bf978",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:lightgreen; font-size:25px\">**PG203 - Deep Learning**</span>\n",
    "\n",
    "Bienvenido al curso!!!\n",
    "\n",
    "Vamos a revisar diferentes algoritmos de <span style=\"color:gold\">aprendizaje profundo</span> y su aplicación en Geología. <br>\n",
    "Es necesario que tengas un conocimiento previo en programación con Python, álgebra lineal, estadística, geología y machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d59dd-9c11-4bbc-9f4a-b3235974af50",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:gold; font-size:20px\">**Redes neuronales desde cero**</span>\n",
    "\n",
    "***\n",
    "- [¿Qué es una red neuronal?](#parte-1)\n",
    "- [Conceptos fundamentales](#parte-2)\n",
    "- [Programando una red neuronal usando Python](#parte-3)\n",
    "- [En conclusión...](#parte-4)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6e3c1-89f5-4240-84f1-2536bb83994d",
   "metadata": {},
   "source": [
    "<a id=\"parte-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f86d85-a24e-4391-ac65-9a6b8c089045",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**¿Qué es una red neuronal?**</span>\n",
    "***\n",
    "\n",
    "Una red neuronal es un modelo computacional <span style=\"color:#43c6ac\">inspirado en el cerebro humano</span>, utilizado en la inteligencia artificial (IA) y el aprendizaje automático (machine learning). \n",
    "\n",
    "Estos modelos estás compuestos por capas de neuronas artificiales interconectadas, que se asemejan a las neuronas biológicas. Su objetivo es aprender patrones y realizar tareas como clasificación, regresión, predicción o generación de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b053332-40f5-46e1-bdaf-7f81d8d864da",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"resources/neural_network_simple.png\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    Fuente: <a href=\"https://towardsdatascience.com/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b\" target=\"_blank\">Towards Data Science</a>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e70de-efe1-4265-b050-dbcbb912c71f",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:gold\">**Breve historia de las redes neuronales**</span>\n",
    "\n",
    "Las redes neuronales artificiales (Artificial Neural Networks o ANN en inglés) tienen sus raíces en la década de 1940, cuando [Warren McCulloch y Walter Pitts](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf) presentaron el primer modelo teórico de una neurona artificial. Inspirados en el funcionamiento del cerebro, su trabajo proponía una forma de <span style=\"color:#43c6ac\">simular el proceso de toma de decisiones a través de una red de unidades interconectadas</span>. \n",
    "\n",
    "En 1958, [Frank Rosenblatt](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) desarrolló el perceptrón, la primera implementación práctica de una red neuronal simple, la cual podía aprender a clasificar patrones mediante ajustes en los pesos de sus conexiones. Aunque este modelo fue limitado en cuanto a lo que podía aprender, abrió el camino para investigaciones posteriores.\n",
    "\n",
    "En las décadas siguientes, el entusiasmo por las redes neuronales fluctuó. Durante los 1970 y 1980, investigadores como [Geoffrey Hinton](https://www.nature.com/articles/323533a0) y [Yann LeCun](https://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) impulsaron nuevas técnicas y arquitecturas, incluyendo la <span style=\"color:gold\">retropropagación</span>, un método que permitió entrenar redes más profundas y complejas, marcando el comienzo de una nueva era. Aun así, las limitaciones computacionales de la época frenaron su progreso hasta el siglo XXI.\n",
    "\n",
    "Con los avances en hardware (como las GPU) y la disponibilidad de grandes conjuntos de datos, las redes neuronales vivieron un renacimiento. En 2012, una red profunda (deep neural network) entrenada por [Alex Krizhevsky, Ilya Sutskever y Geoffrey Hinton](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) ganó la competencia de reconocimiento de imágenes ImageNet, destacando las redes neuronales profundas como una herramienta poderosa para resolver problemas complejos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06f4c3-ee5e-4d09-868f-e3eec5c26bb7",
   "metadata": {},
   "source": [
    "Hoy en día, las redes neuronales son fundamentales para muchas aplicaciones de inteligencia artificial, incluyendo:\n",
    "\n",
    "- **Visión computacional (Computer Vision)**: <br>\n",
    "    Las redes convolucionales (CNN) han revolucionado el análisis de imágenes, permitiendo aplicaciones como la detección de objetos, el diagnóstico médico y el reconocimiento facial.\n",
    "\n",
    "- **Procesamiento del lenguaje natural (Natural Language Processing o NLP)**: <br>\n",
    "    Las redes recurrentes (RNN) y las redes transformadoras (como GPT) han hecho posible la comprensión y generación de texto, la traducción automática y el análisis de sentimientos.\n",
    "\n",
    "- **Robótica (Robotics)**: <br>\n",
    "    Las redes neuronales ayudan en la interpretación de señales visuales y de sensores para la toma de decisiones en tiempo real.\n",
    "\n",
    "- **IA generativa**: <br>\n",
    "    Las redes generativas (GAN) han habilitado la creación de imágenes, música y otros contenidos de alta calidad.\n",
    "\n",
    "Las redes neuronales han evolucionado en alcance y capacidad, permitiendo que muchos sistemas de IA actuales aprendan patrones complejos en los datos y ejecuten tareas que anteriormente se pensaban imposibles para una máquina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e51b94-5de0-4c25-b799-d488bd204312",
   "metadata": {},
   "source": [
    "<a id=\"parte-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c03af-ded0-42e9-8239-1cda622dcf46",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**Conceptos fundamentales**</span>\n",
    "***\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Neurona artificial**</span>: <br>\n",
    "Es la unidad básica de una red neuronal. Cada neurona recibe una serie de entradas (features), las pondera mediante un conjunto de pesos, aplica una función de activación a la suma ponderada y produce una salida. Similar a cómo las neuronas en el cerebro reciben estímulos y generan respuestas.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Pesos (weights)**</span>: <br>\n",
    "Son los parámetros que la red neuronal ajusta durante el proceso de aprendizaje. Indican la importancia de cada entrada en la generación de la salida de una neurona.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Sesgo (bias)**</span>: <br>\n",
    "Es un término adicional que se suma a la combinación ponderada de las entradas antes de aplicar la función de activación. Permite que las neuronas modelen funciones que no pasan por el origen (como en la regresión).\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Inicialización de pesos (weight initialization)**</span>: <br>\n",
    "Asignar valores iniciales a los pesos de la red es fundamental para un aprendizaje efectivo. Métodos como [Xavier](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) y [He](https://arxiv.org/pdf/1502.01852) ajustan los pesos según el número de neuronas, ayudando a evitar problemas como el desvanecimiento del gradiente en redes profundas.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Función de activación (activation function)**</span>: <br>\n",
    "Es la función no lineal que se aplica a la salida de la neurona. Permite a la red aprender representaciones más complejas. Ejemplos de funciones de activación incluyen la función ReLU (Rectified Linear Unit), sigmoide o tangente hiperbólica.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Capas (layers)**</span>: <br>\n",
    "Las neuronas se organizan en capas. Hay tres tipos de capas principales:\n",
    "    - **Capa de entrada**: recibe los datos iniciales.\n",
    "    - **Capas ocultas**: procesan las señales. El número y tamaño de estas capas determinan la capacidad de la red para aprender patrones complejos.\n",
    "    - **Capa de salida**: produce la predicción final.\n",
    "<br> <br>\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Capas totalmente conectadas (fully connected layers)**</span>: <br>\n",
    "En una red neuronal tradicional, cada neurona de una capa está conectada a todas las neuronas de la capa anterior y la capa siguiente. Este tipo de arquitectura se llama feedforward.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Propagación hacia adelante (forward propagation)**</span>: <br>\n",
    "Proceso en el cual los datos se introducen en la capa de entrada, y las señales pasan hacia adelante a través de las capas hasta que se genera una salida final.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Tamaño de lote (batch size)**</span>: <br>\n",
    "El tamaño de lote determina cuántos ejemplos se procesan antes de actualizar los pesos. Batches pequeños aceleran el aprendizaje pero pueden producir gradientes menos estables, mientras que batches grandes dan estabilidad a cambio de un aprendizaje más lento.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Normalización de lotes (batch normalization)**</span>: <br>\n",
    "La normalización de lotes ajusta las activaciones de cada capa en cada mini-lote para estabilizar el entrenamiento y reducir la sensibilidad a la inicialización de pesos. Esto ayuda a usar tasas de aprendizaje más altas y mejora la estabilidad de redes profundas.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Función de pérdida (loss function)**</span>: <br>\n",
    "Es una medida de cuán lejos están las predicciones de los valores reales. Comúnmente usada en redes neuronales, las funciones de pérdida como el error cuadrático medio (MSE) o la entropía cruzada permiten que la red ajuste sus pesos para mejorar las predicciones.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Propagación hacia atrás (backpropagation)**</span>: <br>\n",
    "Algoritmo que ajusta los pesos de la red en función del error de predicción. Usa el descenso de gradiente para minimizar la función de pérdida. El error se calcula en la capa de salida y se retropropaga a través de las capas para actualizar los pesos.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Dropout**</span>: <br>\n",
    "Dropout es una técnica de regularización que “apaga” aleatoriamente neuronas en cada paso de entrenamiento, forzando a la red a aprender representaciones más robustas y evitando que dependa de neuronas específicas. Durante la inferencia, dropout se desactiva.\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Aprendizaje (learning)**</span>: <br>\n",
    "El proceso de ajustar los pesos de la red neuronal para minimizar el error de predicción, lo que se realiza a través de la optimización (usualmente con métodos como gradiente descendente o Adam).\n",
    "\n",
    "- <span style=\"color:lightgreen\">**Sobreajuste (overfitting) y regularización**</span>: <br>\n",
    "Las redes neuronales con demasiados parámetros pueden aprender patrones muy específicos del conjunto de entrenamiento, lo que lleva al overfitting. Técnicas como la regularización L2, el dropout o la early stopping ayudan a prevenir este problema.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752b641-96cc-4b78-b998-de26c3f76d1c",
   "metadata": {},
   "source": [
    "<a id=\"parte-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1c6c4-2b59-4e90-9301-47659c6e8115",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**Programando una red neuronal usando Python**</span>\n",
    "***\n",
    "\n",
    "En este notebook, implementaremos una red neuronal desde cero usando Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eff213-6d02-4304-b0e3-7a574b7317ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Para reproducibilidad\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f52d0c-b9fa-4857-afef-db66f6e959f9",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:gold\">**Preparación de datos**</span>\n",
    "\n",
    "Empezaremos creando un dataset sintético de datos semejantes a una función lineal:\n",
    "\n",
    "- La variable `X` contendrá 100 datos para una sola variable independiente.\n",
    "- La variable `y` contendrá 100 datos de la variable dependiente, más un ruido aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc1a2f-5ecc-415f-ba98-ecbf2782d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generador de números aleatorios\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Dataset para la regresión\n",
    "X = rng.normal(size=(100, 1))                         # 100 datos y una variable\n",
    "y = (3 * X) + 4 + (0.25 * rng.normal(size=(100, 1)))  # Relación lineal con ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d08c3-3b99-4e88-820f-a8b06de3ac72",
   "metadata": {},
   "source": [
    "Procedemos a graficar los datos sintéticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692700c-0c51-4553-95cf-24be0b73970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura principal\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Dispersión de puntos\n",
    "ax.scatter(X, y, s=15, edgecolor=\"k\", lw=.5)\n",
    "\n",
    "# Texto\n",
    "ax.set_title(\"Datos sintéticos lineales\")\n",
    "ax.set_xlabel(\"Variable independiente x\")\n",
    "ax.set_ylabel(\"Variable dependiente y\")\n",
    "\n",
    "# Grilla\n",
    "ax.grid(alpha=.5, c=\"k\", lw=.5, ls=\"--\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6bc54-96a2-4186-9e44-55678f6a6f8d",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:gold\">**Arquitectura de la red neuronal**</span>\n",
    "\n",
    "Nuestra red tendrá una capa de entrada de una sola neurona, una capa oculta con 3 neuronas, una función de activación ReLU, y una capa de salida con una sola neurona para predecir un valor continuo.\n",
    "\n",
    "Empezaremos inicializando los pesos y sesgos usando valores aleatorios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174afea5-1f4b-4372-b7b7-f92a78b74200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de pesos y sesgos\n",
    "input_size = 1    # Una característica (feature) de entrada\n",
    "hidden_size = 3   # 3 neuronas en la capa oculta\n",
    "output_size = 1   # Una salida (regresión)\n",
    "\n",
    "# Primera capa (Entrada -> Oculta)\n",
    "w1 = np.random.randn(input_size, hidden_size) # Pesos para conexiones de entrada a capa oculta\n",
    "b1 = np.zeros((1, hidden_size))               # Sesgos para neuronas de la capa oculta\n",
    "\n",
    "# Segunda capa (Oculta -> Salida)\n",
    "w2 = np.random.randn(hidden_size, output_size) # Pesos para conexiones de capa oculta a capa de salida\n",
    "b2 = np.zeros((1, output_size))                # Sesgos para neuronas de la capa de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc603944-1213-47f1-bff3-f02c8aeb2147",
   "metadata": {},
   "source": [
    "La función ReLU es ampliamente utilizada en las capas ocultas de redes neuronales. Su fórmula es:\n",
    "\n",
    "<center>\n",
    "    $ \\Large \\text{ReLU}(x) = \\text{max}(0, x) $\n",
    "</center>\n",
    "\n",
    "Esta función activa solo las neuronas con valores positivos, introduciendo no linealidad y permitiendo que la red aprenda representaciones más complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424396f6-4263-46bf-961c-32291611a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de activación: ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba6c64-0889-4c1e-84da-1049fa919aa1",
   "metadata": {},
   "source": [
    "La derivada de la función ReLU es esencial para el proceso de retropropagación, ya que determina cómo se ajustarán los pesos en función del error. La derivada de ReLU es:\n",
    "\n",
    "<center>\n",
    "    $ \n",
    "    \\Large\n",
    "    \\text{ReLU}'(x) = \n",
    "    \\begin{cases} \n",
    "    1 & x > 0 \\\\\n",
    "    0 & x \\leq 0 \n",
    "    \\end{cases}\n",
    "    $\n",
    "</center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d3a7b-47ca-46b9-aec5-89c5a8c02fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivada de ReLU\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b60ab-8c39-4487-9951-3a004ef6048a",
   "metadata": {},
   "source": [
    "En este caso, usamos la función identidad para la capa de salida, por lo que el resultado final es simplemente la combinación lineal de las salidas de la capa oculta y sus pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0de2fc-a64d-44f6-a621-768700a5dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identidad (no necesitamos activación no lineal en la salida para regresión)\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c1683-7692-4733-984e-2a42b8357d4d",
   "metadata": {},
   "source": [
    "Ahora, implementaremos la **propagación hacia adelante (forward propagation)** en nuestra red neuronal para calcular las salidas de cada capa.\n",
    "\n",
    "- Primero, calculamos la salida de la capa oculta (`z1`) mediante una multiplicación de la entrada (`X`) por los pesos de la capa (`w1`) y la suma del sesgo (`b1`). \n",
    "\n",
    "- Luego, aplicamos la función de activación ReLU para obtener `a1`, la activación de la capa oculta.\n",
    "\n",
    "- A continuación, usamos `a1` como entrada para la capa de salida, donde calculamos `z2` multiplicando por los pesos de salida (`w2`) y sumando el sesgo (`b2`).\n",
    "\n",
    "- Finalmente, aplicamos la función de activación identidad a `z2` para obtener el `output`, que es la salida final de la red.\n",
    "\n",
    "Esta función devuelve los valores intermedios (`z1`, `a1`, `z2`) y la salida final (`output`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bca8bc-9cdf-437b-ae4a-f581fed32897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagación hacia adelante (forward)\n",
    "def forward(X):\n",
    "    # Cálculo de la capa oculta\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = relu(z1) # Evaluamos con la función de activación\n",
    "    \n",
    "    # Cálculo de la capa de salida\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    output = identity(z2) # Evaluamos con la función de activación\n",
    "    \n",
    "    return z1, a1, z2, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114c533-400a-4859-bbab-deba50c7e68e",
   "metadata": {},
   "source": [
    "Necesitamos de una función de pérdida para medir la precisión de nuestras predicciones. \n",
    "\n",
    "Usaremos el **Error Cuadrático Medio** (**MSE**, por sus siglas en inglés), una métrica común en problemas de regresión. \n",
    "\n",
    "La función `mse_loss` toma dos argumentos: `y_true`, que son los valores reales, y `y_pred`, que son los valores predichos por el modelo. Calcula el error al encontrar la diferencia entre `y_true` y `y_pred`, elevando al cuadrado cada diferencia para penalizar errores grandes, y luego promedia estos valores. Esta métrica nos da una medida de cuán lejos están las predicciones de los valores reales, lo cual es útil para ajustar los pesos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447d286-1793-4193-84f6-de94cb37fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de perdida\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb541ba-d40a-4d98-a880-f46b06abecc8",
   "metadata": {},
   "source": [
    "Finalmente, implementaremos la **propagación hacia atrás** para ajustar los pesos y sesgos de la red en función del error de predicción, utilizando el **descenso de gradiente**. \n",
    "\n",
    "La función `backward` toma la entrada `X`, la salida real `y`, y las salidas intermedias (`z1`, `a1`, `z2`, `output`). \n",
    "\n",
    "- Primero, calcula el error en la capa de salida (`d_output`), que es la derivada de la pérdida (MSE) respecto a la salida predicha.\n",
    "\n",
    "- Luego, calcula los gradientes para los pesos (`w2`) y sesgos (`b2`) de la capa de salida, necesarios para el ajuste.\n",
    "\n",
    "- A continuación, calcula el error en la capa oculta (`d_hidden`) multiplicando `d_output` por los pesos de salida y aplicando la derivada de ReLU a `z1`.\n",
    "\n",
    "- Usando `d_hidden`, calcula los gradientes para los pesos (`w1`) y sesgos (`b1`) de la capa oculta.\n",
    "\n",
    "- Finalmente, ajusta todos los pesos y sesgos en dirección opuesta a los gradientes, escalados por la tasa de aprendizaje (`learning_rate`), para reducir el error en las predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5620f-20d7-483f-9d03-aea7815ff15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagación hacia atrás (backpropagation)\n",
    "def backward(X, y, z1, a1, z2, output, learning_rate=0.01):\n",
    "    # Usar pesos y sesgos actuales\n",
    "    global w1, b1, w2, b2\n",
    "    \n",
    "    # Error en la capa de salida\n",
    "    d_output = 2 * (output - y) / y.size  # Derivada de la pérdida (MSE)\n",
    "    \n",
    "    # Gradientes para W2 y b2\n",
    "    dw2 = np.dot(a1.T, d_output)\n",
    "    db2 = np.sum(d_output, axis=0, keepdims=True)\n",
    "    \n",
    "    # Error en la capa oculta\n",
    "    d_hidden = np.dot(d_output, w2.T) * relu_derivative(z1)\n",
    "    \n",
    "    # Gradientes para W1 y b1\n",
    "    dw1 = np.dot(X.T, d_hidden)\n",
    "    db1 = np.sum(d_hidden, axis=0, keepdims=True)\n",
    "    \n",
    "    # Ajuste de los pesos y sesgos\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b046ef0-c7a6-42f1-a144-c0e099aaef9f",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:gold\">**Entrenamiento de la red neuronal**</span>\n",
    "\n",
    "Una vez definida la arquitectura de la red neuronal, entrenaremos nuestro modelo a través de un bucle de **epochs** (iteraciones).\n",
    "\n",
    "Definimos `epochs` como el número total de iteraciones, y `learning_rate` como la tasa de aprendizaje que determina la magnitud de los ajustes en cada paso.\n",
    "\n",
    "Dentro del bucle:\n",
    "\n",
    "1. <span style=\"color:#43c6ac\">**Propagación hacia adelante**</span>: llamamos a la función `forward` para calcular las salidas de cada capa de la red.\n",
    "\n",
    "2. <span style=\"color:#43c6ac\">**Cálculo de la pérdida**</span>: utilizamos la función `mse_loss` para evaluar la precisión de las predicciones (`output`) comparadas con los valores reales (`y`).\n",
    "\n",
    "3. <span style=\"color:#43c6ac\">**Propagación hacia atrás y ajuste de pesos**</span>: ejecutamos la función `backward`, que ajusta los pesos y sesgos de la red en función de la pérdida y el `learning_rate`.\n",
    "\n",
    "4. <span style=\"color:#43c6ac\">**Visualización del progreso**</span>: cada 50 épocas (o como se desee), imprimimos la pérdida actual para monitorear el progreso del entrenamiento.\n",
    "\n",
    "> El uso de `tqdm` permite mostrar una barra de progreso que facilita ver el avance a lo largo de las épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e015b0-30d1-4b65-a53f-bde2c6cf92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500  # Número de iteraciones\n",
    "learning_rate = 0.05\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Propagación hacia adelante\n",
    "    z1, a1, z2, output = forward(X)\n",
    "    \n",
    "    # Calcular la pérdida\n",
    "    loss = mse_loss(y, output)\n",
    "    \n",
    "    # Propagación hacia atrás y ajuste de pesos\n",
    "    backward(X, y, z1, a1, z2, output, learning_rate)\n",
    "    \n",
    "    # Mostrar el progreso cada 10 épocas\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch: {epoch+1:<3} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c63d9-eeb4-4ed1-991d-53b914c14d3d",
   "metadata": {},
   "source": [
    "Estos resultados muestran cómo la pérdida (error) disminuye progresivamente con cada época de entrenamiento. \n",
    "\n",
    "Al principio, la pérdida es alta (1.3886 en la época 50), pero con cada iteración, la red ajusta sus pesos y reduce la pérdida de manera continua, lo que indica que está aprendiendo a acercarse cada vez más a los valores reales. \n",
    "\n",
    "Al final del entrenamiento (época 500), la pérdida se ha reducido a 0.0782, sugiriendo que la red ha logrado una buena aproximación y las predicciones se acercan a los valores esperados en el conjunto de entrenamiento.\n",
    "\n",
    "Este patrón es común en redes neuronales bien configuradas, donde una reducción de la pérdida sugiere que el modelo está capturando los patrones en los datos sin indicios de sobreajuste en este conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf1e55-8d80-45ae-8eba-2664feb5f626",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:gold\">**Evaluación del modelo**</span>\n",
    "\n",
    "Ahora, debemos realizar una **predicción final** utilizando el conjunto de entrenamiento después de completar todas las épocas de entrenamiento.\n",
    "\n",
    "- Primero, llamamos a la función `forward` con la entrada `X` para obtener la predicción final de la red, almacenada en `y_pred`.\n",
    "\n",
    "- Para evaluar visualmente el rendimiento de la red, vamos a mostrar las primeras cinco predicciones (`y_pred[:5]`) y las comparamos con los valores reales correspondientes (`y[:5]`).\n",
    "\n",
    "> Este paso permite verificar rápidamente si la red está generando predicciones que se acercan a los valores reales en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13955d7-2309-41a1-92cf-9dbc34c4325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción final en el conjunto de entrenamiento\n",
    "_, _, _, y_pred = forward(X)\n",
    "\n",
    "# Mostrar algunas predicciones\n",
    "print(\"Predicciones:\", y_pred[:5].flatten())\n",
    "print(\"Valores reales:\", y[:5].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed922b7-971b-4b36-889f-7a62f59f474a",
   "metadata": {},
   "source": [
    "Para terminar, vamos a visualizar las predicciones de la red neuronal en comparación con los valores reales en el conjunto de entrenamiento mediante un gráfico de dispersión:\n",
    "\n",
    "- Utilizamos `ax.scatter` para representar los valores reales (`y`) en color azul.\n",
    "- Superponemos las predicciones (`y_pred`) en color rojo para comparar visualmente ambas distribuciones.\n",
    "- Añadimos una cuadrícula con `ax.grid` para mejorar la legibilidad, colocamos la leyenda y asignamos un título al gráfico.\n",
    "\n",
    "Este gráfico facilita una evaluación rápida de qué tan bien la red neuronal ha capturado el patrón en los datos al comparar la dispersión de valores reales y predichos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4358b0-da08-48d9-9fe4-ba100734719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura principal\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Dispersión de puntos\n",
    "ax.scatter(X, y, c=\"blue\", s=20, alpha=0.5, edgecolor=\"k\", lw=0.5, label=\"Real\")           # Datos reales\n",
    "ax.scatter(X, y_pred, c=\"red\", s=20, alpha=0.5, edgecolor=\"k\", lw=0.5, label=\"Predicción\") # Predicciones\n",
    "\n",
    "# Texto\n",
    "ax.set_title(\"Evaluación del modelo\")\n",
    "ax.set_xlabel(\"Variable independiente x\")\n",
    "ax.set_ylabel(\"Variable independiente y\")\n",
    "\n",
    "# Grilla\n",
    "ax.grid(lw=0.5, alpha=0.5, c=\"k\", ls=\"--\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Leyenda\n",
    "ax.legend()\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794daf4e-a649-4dc9-8cfc-fb4fdf1ce244",
   "metadata": {},
   "source": [
    "En este gráfico, las predicciones (puntos rojos) parecen ajustarse bien a los valores reales (puntos azules), lo cual indica que el modelo ha capturado de manera efectiva el patrón en los datos. \n",
    "\n",
    "La línea diagonal formada por los puntos muestra una correspondencia cercana entre las predicciones y los valores reales, sugiriendo un buen rendimiento de la red neuronal en el conjunto de entrenamiento.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f46e93-2e2a-40dc-a3cf-9b3eaa35495a",
   "metadata": {},
   "source": [
    "<a id=\"parte-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bea4ea-3b4f-4b41-9d2e-934f86919e50",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**En conclusión...**</span>\n",
    "***\n",
    "\n",
    "- Implementar una red neuronal desde cero en Python permite comprender en profundidad conceptos fundamentales como la propagación hacia adelante, la función de pérdida y la retropropagación, lo cual es esencial para ajustarla y optimizar su rendimiento.\n",
    "\n",
    "- Los hiperparámetros, como la tasa de aprendizaje y el número de épocas, afectan significativamente el rendimiento del modelo. Una tasa de aprendizaje adecuada y suficientes épocas permiten que el modelo converja y capture los patrones en los datos.\n",
    "\n",
    "- Graficar la pérdida a lo largo de las épocas y comparar visualmente las predicciones con los valores reales ayuda a evaluar el rendimiento del modelo, permitiendo identificar problemas como el sobreajuste.\n",
    "\n",
    "- Aunque una implementación básica es funcional, el uso de bibliotecas avanzadas como TensorFlow o PyTorch permite manejar redes neuronales más complejas, optimizar el entrenamiento y experimentar con diferentes arquitecturas para tareas más desafiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1779c-2d1c-4209-8791-3615ad973db9",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
